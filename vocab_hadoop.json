{
    "key": [
        "",
        "*dfs.replication*",
        "*fs.default.name*",
        "*hbase.zookeeper.quorum*",
        "*mapred.job.tracker*",
        "=\nhbase.coprocessor.region.classes",
        "=\nhbase.hregion.majorcompaction",
        "> fs.default.name\n> >> > >>",
        "custom.property",
        "d=\nfs.replication",
        "data.dfs.dir",
        "dfs",
        "dfs.balance.bandwidthPerSec",
        "dfs.block.access.key.update.interval",
        "dfs.block.access.token.enable",
        "dfs.block.access.token.lifetime",
        "dfs.block.replicator.classname",
        "dfs.block.size",
        "dfs.blockreport.initialDelay",
        "dfs.blockreport.split.threshold",
        "dfs.blocksize",
        "dfs.bytes-per-checksum",
        "dfs.client.block.writ=\ne.retries",
        "dfs.client.block.write.replace-datanode-on-failure.best-eff=\nort",
        "dfs.client.block.write.replace-datanode-on-failure.enable",
        "dfs.client.block.write.replace-datanode-on-failure.policy",
        "dfs.client.block.write.retries",
        "dfs.client.cached.conn.retry",
        "dfs.client.context",
        "dfs.client.datanode-restart.timeout",
        "dfs.client.domain.socket.data.traffic",
        "dfs.client.failover.proxy.provider.RhCluster",
        "dfs.client.failover.proxy.provider.bgdt-dev-hrb",
        "dfs.client.failover.proxy.provider.cluster",
        "dfs.client.failover.proxy.provider.hdfs-cluster",
        "dfs.client.failover.proxy.provider.lklcluster",
        "dfs.client.failover.proxy.provider.mycluster",
        "dfs.client.failover.proxy.provider.nameservice1",
        "dfs.client.failover.proxy.provider.nn",
        "dfs.client.failover.proxy.provider.ns1",
        "dfs.client.failover.proxy.provider.orahadoop",
        "dfs.client.failover.proxy.provider.test-cluster",
        "dfs.client.failover.proxy.provider.yisabigdata",
        "dfs.client.https.keystore.resource",
        "dfs.client.mmap.cache.size",
        "dfs.client.mmap.enabled",
        "dfs.client.read.shortcircuit",
        "dfs.client.read.shortcircuit.skip.checksum",
        "dfs.client.read.shortcircuit.streams.cache.expiry.ms",
        "dfs.client.use.datanode.hostname",
        "dfs.client.write.exclude.nodes.cache.expiry.interval.millis=",
        "dfs.cluster.administrators",
        "dfs.da=\ntanode.handler.count",
        "dfs.data.dir",
        "dfs.data.transfer.protection",
        "dfs.datanode.address",
        "dfs.datanode.available-space-volume-choosing-policy.balance=\nd-space-preference-fraction",
        "dfs.datanode.balance.bandwidthPerSec",
        "dfs.datanode.block.id.layout.upgrade.threads",
        "dfs.datanode.cache.revocation.timeout.ms",
        "dfs.datanode.data.dir",
        "dfs.datanode.data.dir.perm",
        "dfs.datanode.dns.interface",
        "dfs.datanode.du.pct",
        "dfs.datanode.du.reserved",
        "dfs.datanode.failed.volumes.tolerated",
        "dfs.datanode.handle=\nr.count",
        "dfs.datanode.handler.count",
        "dfs.datanode.hdfs-blocks-metadata.enabled",
        "dfs.datanode.http.address",
        "dfs.datanode.https.address",
        "dfs.datanode.ipc.address",
        "dfs.datanode.max.transfer.threads",
        "dfs.datanode.max.xc=\nievers",
        "dfs.datanode.max.xciever=\ns",
        "dfs.datanode.max.xcievers",
        "dfs.datanode.name.dir",
        "dfs.datanode.shared.file.descriptor.paths",
        "dfs.datanode.socket.write.tiemout",
        "dfs.datanode.socket.write.timeout",
        "dfs.domain.socket.path",
        "dfs.encrypt.data.transfer",
        "dfs.encrypt.data.transfer.algorithm",
        "dfs.federation.nameservices",
        "dfs.h=\nttp.address",
        "dfs.ha.automatic-failover.enabled",
        "dfs.ha.automatic-failover.enabled.RhCluster",
        "dfs.ha.fencing.methods",
        "dfs.ha.fencing.ssh.connect-timeout",
        "dfs.ha.fencing.ssh.private-key-files",
        "dfs.ha.log-roll.period",
        "dfs.ha.namenode.id",
        "dfs.ha.namenodes.RhCluster",
        "dfs.ha.namenodes.bgdt-dev-hrb",
        "dfs.ha.namenodes.hdfs-cluster",
        "dfs.ha.namenodes.lklcluster",
        "dfs.ha.namenodes.mycluster",
        "dfs.ha.namenodes.nameservice1",
        "dfs.ha.namenodes.ns1",
        "dfs.ha.namenodes.ns2",
        "dfs.ha.namenodes.orahadoop",
        "dfs.ha.namenodes.public-cluster",
        "dfs.ha.namenodes.yisabigdata",
        "dfs.ha.namenodescluster",
        "dfs.ha.tail-edits.period",
        "dfs.heartbeat.interval",
        "dfs.hosts",
        "dfs.hosts.exclude",
        "dfs.http.address",
        "dfs.http.policy",
        "dfs.https.address",
        "dfs.https.enable",
        "dfs.https.port",
        "dfs.https.server.keystore.resource",
        "dfs.image.compress",
        "dfs.info.port",
        "dfs.journalnode.edits.dir",
        "dfs.journalnode.kerberos.https.principal",
        "dfs.journalnode.kerberos.internal.spnego.principal",
        "dfs.journalnode.kerberos.principal",
        "dfs.journalnode.keytab.file",
        "dfs.name.dir",
        "dfs.name.dir.shared0",
        "dfs.name.dir.shared1",
        "dfs.name.edits.dir",
        "dfs.name.edits.dir.shared0",
        "dfs.name.edits.dir.shared1",
        "dfs.namenode.avoid.read.stale.datanode",
        "dfs.namenode.avoid.write.stale.datanode",
        "dfs.namenode.backup.http-address",
        "dfs.namenode.check.stale.datanode",
        "dfs.namenode.checkpoint.dir",
        "dfs.namenode.checkpoint.edits.dir",
        "dfs.namenode.checkpoint.period",
        "dfs.namenode.datanode.registration.ip-hostname-check",
        "dfs.namenode.decommission.nodes.per.interval",
        "dfs.namenode.delegation.key.update-interval",
        "dfs.namenode.delegation.token.max-lifetime",
        "dfs.namenode.delegation.token.renew-interval",
        "dfs.namenode.edits.dir",
        "dfs.namenode.edits.journal-plugin.qjournal",
        "dfs.namenode.edits.noeditlogchannelflush",
        "dfs.namenode.fs-limits.max-blocks-per-file",
        "dfs.namenode.fs-limits.max-component-length",
        "dfs.namenode.fs-limits.max-directory-items",
        "dfs.namenode.fs-limits.max-xattrs-per-inode",
        "dfs.namenode.handler.count",
        "dfs.namenode.hosts",
        "dfs.namenode.http-address",
        "dfs.namenode.http-address.RhCluster.nn1",
        "dfs.namenode.http-address.RhCluster.nn2",
        "dfs.namenode.http-address.bgdt-dev-hrb.nn1",
        "dfs.namenode.http-address.bgdt-dev-hrb.nn2",
        "dfs.namenode.http-address.cluster.nn1",
        "dfs.namenode.http-address.cluster.nn2",
        "dfs.namenode.http-address.hdfs-cluster.nn1",
        "dfs.namenode.http-address.hdfs-cluster.nn2",
        "dfs.namenode.http-address.lklcluster.nn1",
        "dfs.namenode.http-address.lklcluster.nn2",
        "dfs.namenode.http-address.mycluster.nn1",
        "dfs.namenode.http-address.mycluster.nn2",
        "dfs.namenode.http-address.nameservice1.namenode38",
        "dfs.namenode.http-address.nameservice1.namenode90",
        "dfs.namenode.http-address.ns1",
        "dfs.namenode.http-address.ns2",
        "dfs.namenode.http-address.orahadoop.node1",
        "dfs.namenode.http-address.orahadoop.node2",
        "dfs.namenode.http-address.public-cluster.nn1",
        "dfs.namenode.http-address.public-cluster.nn2",
        "dfs.namenode.http-address.yisabigdata.namenode1",
        "dfs.namenode.http-address.yisabigdata.namenode2",
        "dfs.namenode.https-address.bgdt-dev-hrb.nn1",
        "dfs.namenode.https-address.bgdt-dev-hrb.nn2",
        "dfs.namenode.https-address.nameservice1.namenode38",
        "dfs.namenode.https-address.nameservice1.namenode90",
        "dfs.namenode.kerberos.https.principal",
        "dfs.namenode.kerberos.internal.spnego.principal",
        "dfs.namenode.kerberos.principal",
        "dfs.namenode.keytab.file",
        "dfs.namenode.list.cache.pools.num.responses",
        "dfs.namenode.logging.level",
        "dfs.namenode.max.objects",
        "dfs.namenode.name.dir",
        "dfs.namenode.replication.min",
        "dfs.namenode.resource.du.reserved",
        "dfs.namenode.retrycache.expirytime.millis",
        "dfs.namenode.rpc-address.RhCluster.nn1",
        "dfs.namenode.rpc-address.RhCluster.nn2",
        "dfs.namenode.rpc-address.bgdt-dev-hrb.nn1",
        "dfs.namenode.rpc-address.bgdt-dev-hrb.nn2",
        "dfs.namenode.rpc-address.cluster.nn1",
        "dfs.namenode.rpc-address.cluster.nn2",
        "dfs.namenode.rpc-address.clusterA.clust=\nerAnn1",
        "dfs.namenode.rpc-address.hdfs-cluster.nn1",
        "dfs.namenode.rpc-address.hdfs-cluster.nn2",
        "dfs.namenode.rpc-address.lklcluster.nn1",
        "dfs.namenode.rpc-address.lklcluster.nn2",
        "dfs.namenode.rpc-address.mycluster.nn1",
        "dfs.namenode.rpc-address.mycluster.nn2",
        "dfs.namenode.rpc-address.mycluster.nn3",
        "dfs.namenode.rpc-address.nameservice1.namenode38",
        "dfs.namenode.rpc-address.nameservice1.namenode90",
        "dfs.namenode.rpc-address.ns.nn1",
        "dfs.namenode.rpc-address.ns.nn2",
        "dfs.namenode.rpc-address.ns1",
        "dfs.namenode.rpc-address.ns1.nn1",
        "dfs.namenode.rpc-address.ns1.nn3",
        "dfs.namenode.rpc-address.ns2",
        "dfs.namenode.rpc-address.ns2.nn2",
        "dfs.namenode.rpc-address.ns2.nn4",
        "dfs.namenode.rpc-address.orahadoop.node1",
        "dfs.namenode.rpc-address.orahadoop.node2",
        "dfs.namenode.rpc-address.public-cluster.nn1",
        "dfs.namenode.rpc-address.public-cluster.nn2",
        "dfs.namenode.rpc-address.yisabigdata.namenode1",
        "dfs.namenode.rpc-address.yisabigdata.namenode2",
        "dfs.namenode.rpc-bind-host",
        "dfs.namenode.secondary.http-address",
        "dfs.namenode.secondary.http-address.ns1",
        "dfs.namenode.secondary.http-address.ns2",
        "dfs.namenode.secondary.https-address",
        "dfs.namenode.secondaryhttp-address.ns1",
        "dfs.namenode.servicerpc-bind-host",
        "dfs.namenode.shared.edits.dir",
        "dfs.namenode.shared.edits.dir.commoncluster.cnn1",
        "dfs.namenode.shared.edits.dir.commoncluster.cnn2",
        "dfs.namenode.shared.edits.dir.hbasecluster.hnn1",
        "dfs.namenode.shared.edits.dir.hbasecluster.hnn2",
        "dfs.namenode.stale.datanode.interval",
        "dfs.namenode.startup.delay.block.deletion.sec",
        "dfs.namenode.support.allow.format",
        "dfs.namenode.write.stale.datanode.ratio",
        "dfs.nameservices",
        "dfs.nfs.exports.allowed.hosts",
        "dfs.nfs3.export.point",
        "dfs.permissions",
        "dfs.permissions.enabled",
        "dfs.permissions.supergroup",
        "dfs.permissions.superusergroup",
        "dfs.replication",
        "dfs.replication.max",
        "dfs.safemode.extension",
        "dfs.safemode.min.datanodes",
        "dfs.secondary.http.address",
        "dfs.secondary.info.port",
        "dfs.secondary.namenode.kerberos.https.principal",
        "dfs.secondary.namenode.kerberos.internal.spnego.principal",
        "dfs.secondary.namenode.kerberos.principal",
        "dfs.secondary.namenode.keytab.file",
        "dfs.secondaryhttp.address",
        "dfs.socket.timeout",
        "dfs.stream-buffer-size",
        "dfs.support.append",
        "dfs.support.broken.append",
        "dfs.user.home.dir.prefix",
        "dfs.web.authentication.kerberos.keytab",
        "dfs.web.authentication.kerberos.principal",
        "dfs.web.ugi",
        "dfs.webhdfs.enabled",
        "dfs.webhdfs.user.provider.user.pattern",
        "file.blocksize",
        "file.bytes-per-checksum",
        "file.replication",
        "file.stream-buffer-size",
        "fs.AbstractFileSystem.har.impl",
        "fs.AbstractFileSystem.hdfs.impl",
        "fs.automatic.close",
        "fs.checkpoint.dir",
        "fs.checkpoint.period",
        "fs.client.resolve.remote.symlinks",
        "fs.default.nam=\ne\n>",
        "fs.default.nam=\ne\n> >>",
        "fs.default.name",
        "fs.default.name\n> > >>",
        "fs.default.name\n>> > >>",
        "fs.defaultFS",
        "fs.defualt.name",
        "fs.df.interval",
        "fs.du.interval",
        "fs.ftp.host",
        "fs.ftp.host.port",
        "fs.har.impl.disable.cache",
        "fs.hdfs.impl",
        "fs.inmemory.size.mb",
        "fs.permissions.umask-mode",
        "fs.quobyte.impl",
        "fs.s3.awsAccessKeyId",
        "fs.s3.awsSecretAccessKey",
        "fs.s3.block.size",
        "fs.s3.maxRetries",
        "fs.s3a.attempts.maximum",
        "fs.s3a.connection.maximum",
        "fs.s3a.connection.timeout",
        "fs.s3a.impl",
        "fs.s3a.multipart.purge",
        "fs.s3a.paging.maximum",
        "fs.s3n.awsAccessKeyId",
        "fs.s3n.awsSecretAccessKey",
        "fs.s3n.block.size",
        "fs.s3n.impl",
        "fs.s3n.multipart.uploads.enabled",
        "fs.tmp.dir",
        "fs.trash.checkpoint.interval",
        "fs.trash.interval",
        "fs.viewfs.mounttabl=\ne.default.link./home/storage/mount1",
        "fs.viewfs.mounttable.ClusterX.link./dir2",
        "fs.viewfs.mounttable.ClusterX.link./dir3",
        "fs.viewfs.mounttable.ClusterX.link./dir4",
        "fs.viewfs.mounttable.ClusterX.link./dir5",
        "fs.viewfs.mounttable.cluster.link./apachelogs",
        "fs.viewfs.mounttable.cluster.link./datausers",
        "fs.viewfs.mounttable.default.link./NN1Home",
        "fs.viewfs.mounttable.default.link./NN2Home",
        "ftp.blocksize",
        "ftp.bytes-per-checksum",
        "ftp.replication",
        "ftp.stream-buffer-size",
        "h=\nbase.cluster.distributed",
        "ha.failover-controller.graceful-fence.rpc-timeout.ms",
        "ha.health-monitor.connect-retry-interval.ms",
        "ha.health-monitor.rpc-timeout.ms",
        "ha.health-monitor.sleep-after-disconnect.ms",
        "ha.zookeeper.quorum",
        "hadoop.fuse.timer.period",
        "hadoop.http.authentication.cookie.domain",
        "hadoop.http.authentication.kerberos.keytab",
        "hadoop.http.authentication.kerberos.principal",
        "hadoop.http.authentication.signature.secret",
        "hadoop.http.authentication.signature.secret.file",
        "hadoop.http.authentication.simple.anonymous.allowed",
        "hadoop.http.authentication.token.validity",
        "hadoop.http.authentication.type",
        "hadoop.http.filter.initializers",
        "hadoop.job.history.user.location",
        "hadoop.kerberos.kinit.command",
        "hadoop.log.dir",
        "hadoop.logfile.count",
        "hadoop.logfile.size",
        "hadoop.pipes.executable",
        "hadoop.pipes.java.recordreader",
        "hadoop.pipes.java.recordwriter",
        "hadoop.profil=\ne",
        "hadoop.proxyuser.*.groups",
        "hadoop.proxyuser.*.hosts",
        "hadoop.proxyuser.HTTP.groups",
        "hadoop.proxyuser.HTTP.hosts",
        "hadoop.proxyuser.flume.hosts",
        "hadoop.proxyuser.hadoop.hosts",
        "hadoop.proxyuser.hbase.groups",
        "hadoop.proxyuser.hbase.hosts",
        "hadoop.proxyuser.hdfs.groups",
        "hadoop.proxyuser.hdfs.hosts",
        "hadoop.proxyuser.hive.groups",
        "hadoop.proxyuser.hive.hosts",
        "hadoop.proxyuser.hue.groups",
        "hadoop.proxyuser.hue.hosts",
        "hadoop.proxyuser.nfsserver.groups",
        "hadoop.proxyuser.nfsserver.hosts",
        "hadoop.proxyuser.oozie.hosts",
        "hadoop.proxyuser.root.groups",
        "hadoop.proxyuser.root.hosts",
        "hadoop.proxyuser.xeon.groups",
        "hadoop.proxyuser.xeon.hosts",
        "hadoop.proxyuser.yarn.groups",
        "hadoop.proxyuser.yarn.hosts",
        "hadoop.root.logger",
        "hadoop.rpc.protection",
        "hadoop.rpc.socket.factory.class.default",
        "hadoop.security.auth_to_local",
        "hadoop.security.authentication",
        "hadoop.security.authorization",
        "hadoop.security.crypto.cipher.suite",
        "hadoop.security.group.mapping.ldap.directory.search.timeout=",
        "hadoop.security.group.mapping.ldap.search.filter.group",
        "hadoop.security.group.mapping.ldap.search.filter.user",
        "hadoop.security.groups.cache.warn.after.ms",
        "hadoop.security.java.secure.random.algorithm",
        "hadoop.security.kms.client.encrypted.key.cache.low-watermar=\nk",
        "hadoop.security.kms.client.encrypted.key.cache.num.refill.t=\nhreads",
        "hadoop.security.random.device.file.path",
        "hadoop.security.token.service.use_ip",
        "hadoop.security.use-weak-http-crypto",
        "hadoop.ssl.enabled",
        "hadoop.tmp.d=\nir",
        "hadoop.tmp.dir",
        "hadoop.user.group.static.mapping.overrides",
        "hadoop.util.hash.type",
        "hadoop.work.around.non.threadsafe.getpwuid",
        "hadoophack.tunnel.port",
        "hb=\nase.hregion.max.filesize",
        "hb=\nase.master",
        "hb=\nase.zookeeper.property.tickTime",
        "hb=\nase.zookeeper.quorun",
        "hbas=\ne.cluster.distributed",
        "hbase.Coprocessor.region.classes",
        "hbase.ZooKeeper.quorum",
        "hbase.balancer.period",
        "hbase.block.cache.size",
        "hbase.bridge.server.port",
        "hbase.bridge.source.zookeeper.quorum",
        "hbase.bucketcache.bucket.sizes",
        "hbase.bucketcache.combinedcache.enabled",
        "hbase.bucketcache.ioengine",
        "hbase.bucketcache.percentage.in.combinedcache",
        "hbase.bucketcache.size",
        "hbase.bulkload.staging.dir",
        "hbase.cl=\nient.write.buffer",
        "hbase.client.ipc.pool.size",
        "hbase.client.ipc.pool.type",
        "hbase.client.keyvalue.maxsize",
        "hbase.client.pause",
        "hbase.client.prefetch",
        "hbase.client.prefetch.limit",
        "hbase.client.retries.number",
        "hbase.client.s=\ncanner.caching",
        "hbase.client.scanner.**caching",
        "hbase.client.scanner.caching",
        "hbase.client.scanner.max.result.size",
        "hbase.client.scanner.timeout.period",
        "hbase.client.write.buffer",
        "hbase.cluster.****distributed",
        "hbase.cluster.**distributed",
        "hbase.cluster.dist=\nributed",
        "hbase.cluster.distribu=\nted",
        "hbase.cluster.distribute=\nd",
        "hbase.cluster.distributed",
        "hbase.cluster.distributed=",
        "hbase.column.max.version",
        "hbase.coprocessor.master.classes",
        "hbase.coprocessor.region.classes",
        "hbase.coprocessor.user.region.classes",
        "hbase.crypto.keyprovider",
        "hbase.crypto.keyprovider.parameters",
        "hbase.crypto.master.key.name",
        "hbase.data.umask",
        "hbase.data.umask.enable",
        "hbase.defaults.for.version",
        "hbase.defaults.for.version.skip",
        "hbase.hbasemaster.maxregionopen",
        "hbase.hregio=\nn.memstore.mslab.enabled",
        "hbase.hregion.impl",
        "hbase.hregion.majorcompaction",
        "hbase.hregion.max.filesize",
        "hbase.hregion.memcache.block.multiplier",
        "hbase.hregion.memcache.flush.size",
        "hbase.hregion.memstore.block.multiplier",
        "hbase.hregion.memstore.flush.size",
        "hbase.hregion.memstore.mslab.chunksize",
        "hbase.hregion.memstore.mslab.enabled",
        "hbase.hregion.memstore.mslab.max.allocation",
        "hbase.hstore.blockCache.blockSize",
        "hbase.hstore.blockingStoreFiles",
        "hbase.hstore.blockingWaitTime",
        "hbase.hstore.compaction.max",
        "hbase.hstore.compaction.min",
        "hbase.hstore.compaction.throughput.higher.bound",
        "hbase.hstore.compaction.throughput.lower.bound",
        "hbase.hstore.compactionThreshold",
        "hbase.hstore.flush.retries.number",
        "hbase.hstore.flusher.count",
        "hbase.hstore.time.to.purge.deletes",
        "hbase.io.index.interval",
        "hbase.io.seqfile.compression.type",
        "hbase.ipc.c=\nlient.tcpnodelay",
        "hbase.ipc.clien=\nt.tcpnodelay",
        "hbase.ipc.client.tcpnodelay",
        "hbase.ipc.server.callqueue.handler.factor",
        "hbase.ipc.server.read.threadpool.size",
        "hbase.ipc.warn.response.time",
        "hbase.local-file-span-receiver.path",
        "hbase.local.dir",
        "hbase.log.dir",
        "hbase.mapred.tablecolumns",
        "hbase.mas=\nter.info.port",
        "hbase.master",
        "hbase.master.balancer.stochastic.tableSkewCost",
        "hbase.master.distributed.log.splitting",
        "hbase.master.dns.interface",
        "hbase.master.dns.nameserver",
        "hbase.master.handler.count",
        "hbase.master.hfilecleaner.plugins",
        "hbase.master.info.bindAddress",
        "hbase.master.info.port",
        "hbase.master.kerberos.principal",
        "hbase.master.keytab.file",
        "hbase.master.lease.period",
        "hbase.master.lease.thread.wakefrequency",
        "hbase.master.loadbalancer.class",
        "hbase.master.logcleaner.plugins",
        "hbase.master.logcleaner.ttl",
        "hbase.master.meta.thread.rescanfrequency",
        "hbase.master.port",
        "hbase.master.wait.on.regionservers.mintostart",
        "hbase.master.zookeeper.property.tickTime",
        "hbase.master.zookeeper.session.timeout",
        "hbase.metrics.showTableName",
        "hbase.mob.file.cache.size",
        "hbase.mob.sweep.tool.compaction.memstore.flush.size",
        "hbase.nameserver.address",
        "hbase.offheapcache.percentage",
        "hbase.online.schema.update.enable",
        "hbase.regi=\nonserver.impl",
        "hbase.reginserver.restart.on.zk.expire",
        "hbase.regions.percheckin",
        "hbase.regions.slop",
        "hbase.regionserver",
        "hbase.regionserver.\n> logroll.period",
        "hbase.regionserver.\nlogroll.period",
        "hbase.regionserver.cl=\nass",
        "hbase.regionserver.class",
        "hbase.regionserver.codecs",
        "hbase.regionserver.dns.interface",
        "hbase.regionserver.dns.nameserver",
        "hbase.regionserver.executor.closeregion.threads",
        "hbase.regionserver.flushlogentries",
        "hbase.regionserver.global.memstore.lowerLimit",
        "hbase.regionserver.global.memstore.upperLimit",
        "hbase.regionserver.globalMemcache.lowerLimit",
        "hbase.regionserver.globalMemcache.upperLimit",
        "hbase.regionserver.globalMemcacheLimit",
        "hbase.regionserver.globalMemcacheLimitlowMark",
        "hbase.regionserver.handler.count",
        "hbase.regionserver.hlog.reader.impl",
        "hbase.regionserver.hlog.writer.impl",
        "hbase.regionserver.impl",
        "hbase.regionserver.info.bindAddress",
        "hbase.regionserver.info.port",
        "hbase.regionserver.info.port.auto",
        "hbase.regionserver.kerberos.principal",
        "hbase.regionserver.keytab.file",
        "hbase.regionserver.lease.period",
        "hbase.regionserver.logroll.period",
        "hbase.regionserver.maxlogentries",
        "hbase.regionserver.maxlogs",
        "hbase.regionserver.metahandler.count",
        "hbase.regionserver.msginterval",
        "hbase.regionserver.nbreservationblocks",
        "hbase.regionserver.optionalcacheflushinterval",
        "hbase.regionserver.optionallogflushinterval",
        "hbase.regionserver.optionallogrollinterval",
        "hbase.regionserver.p=\nort",
        "hbase.regionserver.port",
        "hbase.regionserver.port=",
        "hbase.regionserver.region.split.policy",
        "hbase.regionserver.regionSplitLimit",
        "hbase.regionserver.safemode",
        "hbase.regionserver.thread.compaction.large",
        "hbase.regionserver.thread.compaction.small",
        "hbase.regionserver.thread.compaction.throttle",
        "hbase.regionserver.thread.split",
        "hbase.regionserver.thread.splitcompactcheckfrequency",
        "hbase.regionserver.thrift.framed",
        "hbase.regionserver.wal.encryption",
        "hbase.replication",
        "hbase.replication.enabled",
        "hbase.rest.port",
        "hbase.rest.readonly",
        "hbase.rest.threads.max",
        "hbase.rest.threads.min",
        "hbase.ro=\notdir",
        "hbase.roo=\ntdir",
        "hbase.rootd=\nir",
        "hbase.rootdi=\nr",
        "hbase.rootdir",
        "hbase.rpc.engine",
        "hbase.rpc.shortoperation.timeout",
        "hbase.rpc.timeout",
        "hbase.security.authentication",
        "hbase.security.authorization",
        "hbase.server.thread.wakefrequency",
        "hbase.snapshot.enabled",
        "hbase.snapshot.master.timeout.millis",
        "hbase.snapshot.master.timeoutMillis",
        "hbase.snapshot.region.timeout",
        "hbase.snapshot.restore.take.failsafe.snapshot",
        "hbase.splitlog.manager.timeout",
        "hbase.status.listener.class",
        "hbase.status.published",
        "hbase.status.publisher.class",
        "hbase.storescanner.parallel.seek.enable",
        "hbase.superuser",
        "hbase.table.archive.directory",
        "hbase.table.lock.enable",
        "hbase.tmp.dir",
        "hbase.trace.spanreceiver.classes",
        "hbase.wal.provider",
        "hbase.wal.regiongrouping.numgroups",
        "hbase.zookeepe=\nr.quorum",
        "hbase.zookeeper.distributed",
        "hbase.zookeeper.dns.interface",
        "hbase.zookeeper.dns.nameserver",
        "hbase.zookeeper.leaderport",
        "hbase.zookeeper.peerport",
        "hbase.zookeeper.propert=\ny.dataDir",
        "hbase.zookeeper.property.clientPort",
        "hbase.zookeeper.property.dataDir",
        "hbase.zookeeper.property.initLimit",
        "hbase.zookeeper.property.maxClientCnxns",
        "hbase.zookeeper.property.maxSessionTimeout",
        "hbase.zookeeper.property.requireClientAuthScheme",
        "hbase.zookeeper.property.syncLimit",
        "hbase.zookeeper.property.tickTime",
        "hbase.zookeeper.quor=\num",
        "hbase.zookeeper.quorom",
        "hbase.zookeeper.quorum",
        "hbase.zookeeper.quorun",
        "hbase.zookeeper.useMulti",
        "hbase=\n.hregion.memstore.mslab.max.allocation",
        "hbase=\n.zookeeper.property.tickTime",
        "hbase=\n.zookeeper.quorun",
        "hbaseshell.jline.bell.enabled",
        "hdfs.root",
        "hfile.block.cache.size",
        "hfile.format.version",
        "hfile.min.blocksize.size",
        "hive.aux.jars.path",
        "hive.exec.scratchdir",
        "hive.metast=\nore.uris",
        "hive.metastore.uris",
        "hive.metastore.warehouse.dir",
        "hive.querylog.location",
        "hive.support.concurrency",
        "httpfs.fsAccess.conf:fs.default.name",
        "httpfs.fsAccess.conf:fs.default.name\n>",
        "httpfs.fsAccess.conf:fs.default.name\n> >",
        "httpfs.fsAccess.conf:fs.default.name\n>> >",
        "httpfs.fsAccess.conf:fs.default.name\n>>> >",
        "httpfs.fsAccess.conf:fs.default.name\n>>>> >",
        "instance.volumes",
        "io.compression.codec.bzip2.library",
        "io.compression.codec.lzo.class",
        "io.compression.codecs",
        "io.file.buffer.size",
        "io.map.index.interval",
        "io.map.index.skip",
        "io.mapfile.bloom.size",
        "io.native.lib.available",
        "io.seqfile.lazydecompress",
        "io.seqfile.local.dir",
        "io.serializations",
        "io.skip.checksum.errors",
        "io.sort.factor",
        "io.sort.mb",
        "io.sort.record.percent",
        "io.sort.spill.percent",
        "ipc.client.connect.retry.interval",
        "ipc.client.connection.maxidletime",
        "ipc.client.fallback-to-simple-auth-allowed",
        "ipc.client.idlethreshold",
        "ipc.client.kill.max",
        "ipc.client.tcpnodelay",
        "ipc.server.tcpnodelay",
        "java.library.path",
        "javax.jdo.option.ConnectionDriverName",
        "javax.jdo.option.ConnectionPassword",
        "javax.jdo.option.ConnectionURL",
        "javax.jdo.option.ConnectionUser=\nName",
        "javax.jdo.option.ConnectionUserName",
        "javax.xml.parsers.DocumentBuilderFa=\nctory",
        "javax.xml.parsers.DocumentBuilderFac=\ntory",
        "javax.xml.parsers.DocumentBuilderFactory",
        "jobtracker.thrift.address",
        "keep.failed.task.files",
        "keep.task.files.pattern",
        "local.cache.size",
        "m=\napred.tasktracker.map.tasks.maximum",
        "map.sort.class",
        "mapre=\nd.reduce.child.java.opts",
        "mapred.=\nfairscheduler.poolnameproperty",
        "mapred.=\nfairscheduler.preemption",
        "mapred.acls.enabled",
        "mapred.child.env",
        "mapred.child.java.opts",
        "mapred.child.tmp",
        "mapred.child.ulimit",
        "mapred.compress.map.output",
        "mapred.fairscheduler.allocation.file",
        "mapred.fairscheduler.allow.undeclared.pools",
        "mapred.fairscheduler.poolnameproperty",
        "mapred.fairscheduler.preemption",
        "mapred.framework.name",
        "mapred.healthChecker.script.timeout",
        "mapred.hosts",
        "mapred.hosts.exclude",
        "mapred.input.format.class",
        "mapred.job.classpath.archives",
        "mapred.job.reuse.jvm.num.tasks",
        "mapred.job.shuffle.merge.percent",
        "mapred.job.tracker",
        "mapred.job.tracker.handler.count",
        "mapred.job.tracker.http.address",
        "mapred.job.tracker.info.port",
        "mapred.job.tracker=",
        "mapred.jobtracker.blacklist.fault-timeout-window",
        "mapred.jobtracker.plugins",
        "mapred.jobtracker.taskScheduler",
        "mapred.local.dir",
        "mapred.map.child.java.opts",
        "mapred.map.output.compress.codec",
        "mapred.map.output.compression.codec",
        "mapred.map.tasks",
        "mapred.map.tasks.speculative.execution",
        "mapred.mapper.new-api",
        "mapred.output.compress",
        "mapred.output.compression.codec",
        "mapred.output.compression.type",
        "mapred.queue.=\nnames",
        "mapred.queue.default.acl-administer-jobs",
        "mapred.queue.default.acl-submit-job",
        "mapred.queue.name",
        "mapred.queue.names",
        "mapred.queue.q_test.acl-submit-job",
        "mapred.queue.sqoop.acl-administer-jobs",
        "mapred.queue.sqoop.acl-submit-job",
        "mapred.reduce.child.java.opts",
        "mapred.reduce.max.attempts",
        "mapred.reduce.parallel.copies",
        "mapred.reduce.slowstart.completed.maps",
        "mapred.reduce.tasks",
        "mapred.reduce.tasks.speculative.execution",
        "mapred.reducer.new-api",
        "mapred.submit.replication",
        "mapred.system.dir",
        "mapred.task.id",
        "mapred.task.profile",
        "mapred.task.profile.maps",
        "mapred.task.profile.reduces",
        "mapred.task.timeout",
        "mapred.task.tracker.http.address",
        "mapred.task.tracker.report.address",
        "mapred.tasktacker.map.tasks.maximum",
        "mapred.tasktacker.reduce.tasks.maximum",
        "mapred.tasktracker.expiry.interval",
        "mapred.tasktracker.map.tasks.maximum",
        "mapred.tasktracker.red=\nuce.tasks.maximum",
        "mapred.tasktracker.reduce.tasks.maximum",
        "mapred.textoutputformat.separator",
        "mapred.userlog.retain.hours",
        "mapreduce.admin.map.child.java.opts",
        "mapreduce.admin.reduce.child.java.opts",
        "mapreduce.admin.user.env",
        "mapreduce.am.max-attempts",
        "mapreduce.application.classpath",
        "mapreduce.child.java.opts",
        "mapreduce.client.genericoptionsparser.used",
        "mapreduce.client.progressmonitor.pollinterval",
        "mapreduce.cluster.local.dir",
        "mapreduce.cluster.temp.dir",
        "mapreduce.fi=\nleoutputcommitter.marksuccessfuljobs",
        "mapreduce.fileoutputcommitter.marksuccessfuljob=\ns",
        "mapreduce.fileoutputcommitter.marksuccessfuljobs",
        "mapreduce.framework.name",
        "mapreduce.ifile.readahead",
        "mapreduce.input.fileinputformat.split.minsize",
        "mapreduce.j=\nob.acl-view-job",
        "mapreduce.job.acl-modify-job",
        "mapreduce.job.acl-view-job",
        "mapreduce.job.classloader.system.classes",
        "mapreduce.job.committer.setup.cleanup.needed",
        "mapreduce.job.counters.max",
        "mapreduce.job.end-notification.retry.attempts",
        "mapreduce.job.end-notification.retry.interval",
        "mapreduce.job.jvm.numtasks",
        "mapreduce.job.map.output.collector.class",
        "mapreduce.job.maps",
        "mapreduce.job.reduce.slowstart.completedmaps",
        "mapreduce.job.reduces",
        "mapreduce.job.speculative.slownodethreshold",
        "mapreduce.job.speculative.slowtaskthreshold",
        "mapreduce.job.split.metainfo.maxsize",
        "mapreduce.job.token.tracking.ids.enabled",
        "mapreduce.job.tracker.address",
        "mapreduce.jobhistory.address",
        "mapreduce.jobhistory.admin.address",
        "mapreduce.jobhistory.cleaner.interval-ms",
        "mapreduce.jobhistory.datestring.cache.size",
        "mapreduce.jobhistory.done-dir",
        "mapreduce.jobhistory.intermediate-done-dir",
        "mapreduce.jobhistory.keytab",
        "mapreduce.jobhistory.max-age-ms",
        "mapreduce.jobhistory.minicluster.fixed.ports",
        "mapreduce.jobhistory.principal",
        "mapreduce.jobhistory.recovery.store.fs.uri",
        "mapreduce.jobhistory.webapp.address",
        "mapreduce.jobtracker.addre=\nss",
        "mapreduce.jobtracker.addre=\nssr",
        "mapreduce.jobtracker.address",
        "mapreduce.jobtracker.expire.trackers.interval",
        "mapreduce.jobtracker.hosts.filename",
        "mapreduce.jobtracker.http.address",
        "mapreduce.jobtracker.jobhistory.block.size",
        "mapreduce.jobtracker.jobhistory.location",
        "mapreduce.jobtracker.kerberos.https.principal",
        "mapreduce.jobtracker.kerberos.principal",
        "mapreduce.jobtracker.persist.jobstatus.active",
        "mapreduce.jobtracker.persist.jobstatus.dir",
        "mapreduce.jobtracker.persist.jobstatus.hours",
        "mapreduce.jobtracker.retiredjobs.cache.size",
        "mapreduce.jobtracker.split.metainfo.maxsize",
        "mapreduce.jobtracker.staging.root.dir",
        "mapreduce.jobtracker.system.dir",
        "mapreduce.map.cpu.vcores",
        "mapreduce.map.java.opts",
        "mapreduce.map.log.level",
        "mapreduce.map.maxattempts",
        "mapreduce.map.memory.mb",
        "mapreduce.map.output.compress",
        "mapreduce.map.output.compress.codec",
        "mapreduce.map.speculative",
        "mapreduce.output.fileoutputformat.compress",
        "mapreduce.reduce.cpu.vcores",
        "mapreduce.reduce.java.opts",
        "mapreduce.reduce.log.level",
        "mapreduce.reduce.maxattempts",
        "mapreduce.reduce.memory.mb",
        "mapreduce.reduce.merge.inmem.threshold",
        "mapreduce.reduce.shuffle.connect.timeout",
        "mapreduce.reduce.shuffle.input.buffer.percent",
        "mapreduce.reduce.shuffle.maxfetchfailures",
        "mapreduce.reduce.shuffle.memory.limit.percent",
        "mapreduce.reduce.shuffle.parallelcopies",
        "mapreduce.reduce.shuffle.read.timeout",
        "mapreduce.reduce.shuffle.retry-delay.max.ms",
        "mapreduce.reduce.skip.proc.count.autoincr",
        "mapreduce.reduce.speculative",
        "mapreduce.shuffle.port",
        "mapreduce.task.attempt.id",
        "mapreduce.task.files.preserve.failedtasks",
        "mapreduce.task.io.sort.factor",
        "mapreduce.task.io.sort.mb",
        "mapreduce.task.profile",
        "mapreduce.task.profile.map.params",
        "mapreduce.task.profile.maps",
        "mapreduce.task.profile.params",
        "mapreduce.task.timeout",
        "mapreduce.task.tmp.dir",
        "mapreduce.task.userlog.limit.kb",
        "mapreduce.tasktracker.healthchecker.interval",
        "mapreduce.tasktracker.http.address",
        "mapreduce.tasktracker.indexcache.mb",
        "mapreduce.tasktracker.local.dir.minspacekill",
        "mapreduce.tasktracker.local.dir.minspacestart",
        "mapreduce.tasktracker.map.tasks.maximum",
        "mapreduce.tasktracker.outofband.heartbeat",
        "mapreduce.tasktracker.reduce.tasks.maximum",
        "mapreduce.tasktracker.report.address",
        "net.topology.impl",
        "net.topology.script.file.name",
        "nfs.allow.insecure.ports",
        "nfs.dump.dir",
        "nfs.exports.allowed.hosts",
        "nfs.mountd.port",
        "nfs.rtmax",
        "nfs.server.port",
        "nfs.wtmax",
        "oozie.launcher.mapred.fairscheduler.pool",
        "param1",
        "replication.replicationsource.implementation",
        "replication.source.nb.capacity",
        "replication.source.ratio",
        "s3.blocksize",
        "s3.replication",
        "s3native.blocksize",
        "s3native.replication",
        "security.admin.protocol.acl",
        "security.client.datanode.protocol.acl",
        "security.client.protocol.acl",
        "security.inter.tracker.protocol.acl",
        "security.masterregion.protocol.acl",
        "tasktracker.http.port",
        "tasktracker.http.threads",
        "test.SymlinkEnabledForTesting",
        "tfile.io.chunk.size",
        "tickTime",
        "topology.node.switch.mapping.impl",
        "topology.script.file.name",
        "topology.script.number.args",
        "yarn.acl.enable",
        "yarn.admin.acl",
        "yarn.am.liveness-monitor.expiry-interval-ms",
        "yarn.app.mapreduce.am.admin-command-opts",
        "yarn.app.mapreduce.am.admin.user.env",
        "yarn.app.mapreduce.am.command-opts",
        "yarn.app.mapreduce.am.container.log.limit.kb",
        "yarn.app.mapreduce.am.env",
        "yarn.app.mapreduce.am.job.client.port-range",
        "yarn.app.mapreduce.am.job.task.listener.thread-count",
        "yarn.app.mapreduce.am.log.level",
        "yarn.app.mapreduce.am.resource.mb",
        "yarn.app.mapreduce.am.staging-dir",
        "yarn.app.mapreduce.client-am.ipc.max-retries",
        "yarn.app.mapreduce.task.container.log.backups",
        "yarn.application.classpath",
        "yarn.application.classpath.ext",
        "yarn.client.failover-proxy-provider",
        "yarn.client.failover-retries",
        "yarn.client.nodemanager-connect.max-wait-ms",
        "yarn.http.policy",
        "yarn.ipc.rpc.class",
        "yarn.log-aggregation-enable",
        "yarn.log-aggregation.retain-check-interval-seconds",
        "yarn.log-aggregation.retain-seconds",
        "yarn.log.server.url",
        "yarn.nm.liveness-monitor.expiry-interval-ms",
        "yarn.nodemanager.address",
        "yarn.nodemanager.admin-env",
        "yarn.nodemanager.aux-services",
        "yarn.nodemanager.aux-services.bar.class",
        "yarn.nodemanager.aux-services.foo.class",
        "yarn.nodemanager.aux-services.mapreduce.shuffle.class",
        "yarn.nodemanager.aux-services.mapreduce_shuffle.class",
        "yarn.nodemanager.container-executor.class",
        "yarn.nodemanager.container-monitor.resource-calculator.class",
        "yarn.nodemanager.delete.debug-delay-sec",
        "yarn.nodemanager.delete.thread-count",
        "yarn.nodemanager.disk-health-checker.min-free-space-per-dis=\nk-mb",
        "yarn.nodemanager.disk-health-checker.min-healthy-disks",
        "yarn.nodemanager.health-checker.interval-ms",
        "yarn.nodemanager.health-checker.script.opts",
        "yarn.nodemanager.health-checker.script.path",
        "yarn.nodemanager.linux-container-executor.group",
        "yarn.nodemanager.linux-container-executor.resources-handler=\n.class",
        "yarn.nodemanager.local-*dirs*",
        "yarn.nodemanager.local-cache.max-files-per-directory",
        "yarn.nodemanager.local-dirs",
        "yarn.nodemanager.localizer.address",
        "yarn.nodemanager.localizer.fetch.thread-count",
        "yarn.nodemanager.log-aggregation-enable",
        "yarn.nodemanager.log-dirs",
        "yarn.nodemanager.log.dirs",
        "yarn.nodemanager.log.retain-seconds",
        "yarn.nodemanager.recovery.dir",
        "yarn.nodemanager.remote-app-log-dir",
        "yarn.nodemanager.remote-app-log-dir-suffix",
        "yarn.nodemanager.resource.cpu-cores",
        "yarn.nodemanager.resource.cpu-vcores",
        "yarn.nodemanager.resource.memory-mb",
        "yarn.nodemanager.resourcemanager.connect.retry_interval.sec=\ns",
        "yarn.nodemanager.vcores-pcores-ratio",
        "yarn.nodemanager.vmem-pmem-ratio",
        "yarn.nodemanager.webapp.address",
        "yarn.resourcemanager.address",
        "yarn.resourcemanager.admin.address",
        "yarn.resourcemanager.am.max-attempts",
        "yarn.resourcemanager.bind-host",
        "yarn.resourcemanager.client.thread-count",
        "yarn.resourcemanager.connect.max-wait.ms",
        "yarn.resourcemanager.container.liveness-monitor.interval-ms=",
        "yarn.resourcemanager.delayed.delegation-token.removal-interval-ms",
        "yarn.resourcemanager.hostname",
        "yarn.resourcemanager.nm.liveness-monitor.interval-ms",
        "yarn.resourcemanager.nodes.exclude-path",
        "yarn.resourcemanager.proxy-user-privileges.enabled",
        "yarn.resourcemanager.resource-tracker.address",
        "yarn.resourcemanager.scheduler.address",
        "yarn.resourcemanager.scheduler.class",
        "yarn.resourcemanager.scheduler.monitor.enable",
        "yarn.resourcemanager.webapp.address",
        "yarn.resourcemanager.webapp.delegation-token-auth-filter.en=\nabled",
        "yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled",
        "yarn.resourcemanager.zk-acl",
        "yarn.resourcemanager.zk-retry-interval-ms",
        "yarn.root.logger",
        "yarn.scheduler.capacity.maximum-am-resource-percent",
        "yarn.scheduler.capacity.maximum-applications",
        "yarn.scheduler.capacity.node-locality-delay",
        "yarn.scheduler.capacity.queue-mappings",
        "yarn.scheduler.capacity.queue-mappings-override.enable",
        "yarn.scheduler.capacity.resource-calculator",
        "yarn.scheduler.capacity.root.\n> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\n>>>>>>>>>>>>>>>>>> production.acl_submit_applications",
        "yarn.scheduler.capacity.root.\nproduction.acl_submit_applications",
        "yarn.scheduler.capacity.root.*dev*\n.acl_submit_applications",
        "yarn.scheduler.capacity.root.*dev*\n> .acl_submit_applications",
        "yarn.scheduler.capacity.root.acl_administer_queue",
        "yarn.scheduler.capacity.root.acl_submit_applications",
        "yarn.scheduler.capacity.root.capacity",
        "yarn.scheduler.capacity.root.default.acl_administer_queue",
        "yarn.scheduler.capacity.root.default.acl_submit_applications",
        "yarn.scheduler.capacity.root.default.capacity",
        "yarn.scheduler.capacity.root.default.maximum-capacity",
        "yarn.scheduler.capacity.root.default.minimum-user-limit-percent",
        "yarn.scheduler.capacity.root.default.state",
        "yarn.scheduler.capacity.root.default.user-limit-factor",
        "yarn.scheduler.capacity.root.dev.acl_submit_applications",
        "yarn.scheduler.capacity.root.exploration.a.acl_submit_applications",
        "yarn.scheduler.capacity.root.exploration.a.capacity",
        "yarn.scheduler.capacity.root.exploration.b.acl_submit_applications",
        "yarn.scheduler.capacity.root.exploration.b.capacity",
        "yarn.scheduler.capacity.root.exploration.c.acl_submit_applications",
        "yarn.scheduler.capacity.root.exploration.c.capacity",
        "yarn.scheduler.capacity.root.exploration.capacity",
        "yarn.scheduler.capacity.root.exploration.queues",
        "yarn.scheduler.capacity.root.production.capacity",
        "yarn.scheduler.capacity.root.queues",
        "yarn.scheduler.capacity.root.small.capacity",
        "yarn.scheduler.capacity.root.small.maximum-am-resource-percent",
        "yarn.scheduler.capacity.root.small.maximum-capacity",
        "yarn.scheduler.capacity.root.small.user-limit",
        "yarn.scheduler.maximum-allocation-mb",
        "yarn.scheduler.maximum-allocation-vcores",
        "yarn.scheduler.minimum-allocation-mb",
        "yarn.scheduler.minimum-allocation-vcores",
        "yarn.timeline-service.address",
        "yarn.timeline-service.hostname",
        "yarn.timeline-service.leveldb-timeline-store.ttl-interval-m=\ns",
        "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms",
        "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=",
        "yarn.timeline-service.store-class",
        "yarn.timeline-service.ttl-ms",
        "yarn.web-proxy.address",
        "zlib.compress.level",
        "zooke=\neper.session.timeout",
        "zookee=\nper.session.timeout",
        "zookeeper.retries",
        "zookeeper.sessio=\nn.timeout",
        "zookeeper.session.timeout",
        "zookeeper.znode.acl.parent",
        "zookeeper.znode.master",
        "zookeeper.znode.parent",
        "zookeeper.znode.rootserver"
    ],
    "value": [
        "\n",
        "\n\norg.apache.hadoop.io.compress.GzipCodec,\n\norg.apache.hadoop.io.compress.DefaultCodec,\n\norg.apache.hadoop.io.compress.BZip2Codec,\n\norg.apache.hadoop.io.compress.SnappyCodec\n                                       ",
        "\n        \norg.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer\n        ",
        "\n                 /scratch/usr/software/hadoop2/hadoop-dc/temp/nm-local-dir,\n                /tmp/nm-local-dir\n     ",
        "\n        $HADOOP_CONF_DIR, $HADOOP_COMMON_HOME/share/hadoop/common/*,\n$HADOOP_COMMON_HOME/share/hadoop/common/lib/*, $HADO$\n     ",
        "\n        org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServ=\ner\n        ",
        "\n      org.apache.hadoop.io.compress.GzipCodec,\n      org.apache.hadoop.io.compress.DefaultCodec,\n      org.apache.hadoop.io.compress.BZip2Codec,\n      org.apache.hadoop.io.compress.SnappyCodec\n    ",
        "\n      sshfence\n      shell(/bin/true)\n    ",
        "\n     -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc\n-Xloggc:/tmp/@taskid@.gc\n     -Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n  ",
        "\n     -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc\n-Xloggc:/tmp/@taskid@.gc\n     -Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n  ",
        "\n$HADOOP_CONF_DIR,\n$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,\n      $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,\n$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,\n$YARN_HOME/*,$YARN_HOME/lib/*\n",
        "\n-Xmx512m -Djava.net.preferIPv4Stack=true -XX:+UseCompressedOops\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/home/sfdc/logs/hadoop/userlogs/@taskid@/\n",
        "\n/etc/hadoop/conf,/usr/lib/hadoop/*,/usr/lib/hadoop/lib/*,/usr/lib/hadoop-hdfs/*,/usr/lib/hadoop-hdfs/lib/*,/usr/lib/hadoop-yarn/*,/usr/lib/hadoop-yarn/lib/*,/usr/lib/hadoop-mapreduce/*,/usr/lib/hadoop-mapreduce/lib/*,/home/hduser/mahout-1.0-snapshot/math/target/*\n         ",
        "\n>\n> org.apache.hadoop.io.compress.GzipCodec,\n>\n> org.apache.hadoop.io.compress.DefaultCodec,\n>\n> org.apache.hadoop.io.compress.BZip2Codec,\n>\n> org.apache.hadoop.io.compress.SnappyCodec\n>                                        ",
        "\n> ",
        "\n>                  /scratch/usr/software/hadoop2/hadoop-dc/temp/nm-local-dir,\n>                 /tmp/nm-local-dir\n>      ",
        "\n>                 -Djava.library.path=/usr/local/hadoop-2.4.0/lib/native",
        "\n>             -Djava.library.path=/usr/local/hadoop-2.4.0/lib/native",
        "\n>         $HADOOP_CONF_DIR, $HADOOP_COMMON_HOME/share/hadoop/common/*, $HAD=\nOOP_COMMON_HOME/share/hadoop/common/lib/*, $HADO$\n>      ",
        "\n>       sshfence\n>       shell(/bin/true)\n>     ",
        "\n>   -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc\n> -Xloggc:/tmp/@taskid@.gc\n>   -Dcom.sun.management.jmxremote.authenticate=false\n> -Dcom.sun.management.jmxremote.ssl=false\n> ",
        "\n>   -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc\n> -Xloggc:/tmp/@taskid@.gc\n>   -Dcom.sun.management.jmxremote.authenticate=false\n> -Dcom.sun.management.jmxremote.ssl=false\n> ",
        "\n> $HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,$HADOOP_COMMON=\n_HOME/share/hadoop/common/lib/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADO=\nOP_HDFS_HOME/share/hadoop/hdfs/lib/*,$HADOOP_YARN_HOME/share/hadoop/yarn/*,=\n$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*\n> ",
        "\n> -Xmx512m -Djava.net.preferIPv4Stack=3Dtrue -XX:+UseCompressedOops =\n-XX:+HeapDumpOnOutOfMemoryError =\n-XX:HeapDumpPath=3D/home/sfdc/logs/hadoop/userlogs/@taskid@/\n> ",
        "\n> -Xmx512m -Djava.net.preferIPv4Stack=true -XX:+UseCompressedOops\n> -XX:+HeapDumpOnOutOfMemoryError\n> -XX:HeapDumpPath=/home/sfdc/logs/hadoop/userlogs/@taskid@/\n> ",
        "\n> /etc/hadoop/conf,/usr/lib/hadoop/*,/usr/lib/hadoop/lib/*,/usr/lib/hadoop-hdfs/*,/usr/lib/hadoop-hdfs/lib/*,/usr/lib/hadoop-yarn/*,/usr/lib/hadoop-yarn/lib/*,/usr/lib/hadoop-mapreduce/*,/usr/lib/hadoop-mapreduce/lib/*,/home/hduser/mahout-1.0-snapshot/math/target/* \n>\n>         ",
        "\n> =A0 =A0 =A0 =A0org.apache.hadoop.hbase.regionserver.tableindexed.IndexedR=\negionServer\n> =A0 =A0 =A0 =A0",
        "\n> >\n> > org.apache.hadoop.io.compress.GzipCodec,\n> >\n> > org.apache.hadoop.io.compress.DefaultCodec,\n> >\n> > org.apache.hadoop.io.compress.BZip2Codec,\n> >\n> > org.apache.hadoop.io.compress.SnappyCodec\n> >                                        ",
        "\n> > ",
        "\n> >       sshfence\n> >       shell(/bin/true)\n> >     ",
        "\n> >> >       sshfence\n> >> >       shell(/bin/true)\n> >> >     ",
        "\n>>         $HADOOP_CONF_DIR, $HADOOP_COMMON_HOME/share/hadoop/common/*, $HA=\nDOOP_COMMON_HOME/share/hadoop/common/lib/*, $HADO$\n>>      ",
        "\n>> -Xmx512m -Djava.net.preferIPv4Stack=true -XX:+UseCompressedOops\n>> -XX:+HeapDumpOnOutOfMemoryError\n>> -XX:HeapDumpPath=/home/sfdc/logs/hadoop/userlogs/@taskid@/\n>> ",
        "\n>> /etc/hadoop/conf,/usr/lib/hadoop/*,/usr/lib/hadoop/lib/*,/usr/lib/hadoop-hdfs/*,/usr/lib/hadoop-hdfs/lib/*,/usr/lib/hadoop-yarn/*,/usr/lib/hadoop-yarn/lib/*,/usr/lib/hadoop-mapreduce/*,/usr/lib/hadoop-mapreduce/lib/*,/home/hduser/mahout-1.0-snapshot/math/target/* \n>>\n>>         ",
        "\n>> =A0 =A0 =A0 =A0org.apache.hadoop.hbase.regionserver.tableindexed.Indexed=\nRegionServer\n>> =A0 =A0 =A0 =A0",
        "\n>> >\n>> > org.apache.hadoop.io.compress.GzipCodec,\n>> >\n>> > org.apache.hadoop.io.compress.DefaultCodec,\n>> >\n>> > org.apache.hadoop.io.compress.BZip2Codec,\n>> >\n>> > org.apache.hadoop.io.compress.SnappyCodec\n>> >                                        ",
        "\n>> >       sshfence\n>> >       shell(/bin/true)\n>> >     ",
        "\n>>> -Xmx512m -Djava.net.preferIPv4Stack=true -XX:+UseCompressedOops\n>>> -XX:+HeapDumpOnOutOfMemoryError\n>>> -XX:HeapDumpPath=/home/sfdc/logs/hadoop/userlogs/@taskid@/\n>>> ",
        "\n>>> =A0 =A0 =A0 =A0org.apache.hadoop.hbase.regionserver.tableindexed.Indexe=\ndRegionServer\n>>> =A0 =A0 =A0 =A0",
        "\n>>>>         \n>>>> org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer\n>>>>        ",
        "\nlocalhost:54311",
        " ",
        " \n-Djava.library.path=/usr/local/hadoop-2.4.0/lib/native",
        " \n> -Djava.library.path=/usr/local/hadoop-2.4.0/lib/native",
        " \n>> -Djava.library.path=/usr/local/hadoop-2.4.0/lib/native",
        " \n>>> -Djava.library.path=/usr/local/hadoop-2.4.0/lib/native",
        " -Xmx1073741824",
        " -Xmx471075479",
        " /data01/workspace/hive scratch/dir/on/local/linux/disk",
        " hdfs,hdpdadmngrp",
        " main.java.coprocessor.RowCountEndPoint",
        " org.apache.hadoop.mapred.ShuffleHandler",
        " something ",
        " testgroup",
        " true",
        " webgroup",
        "\"LD_LIBRARY_PATH=3D/usr/lib64:/usr/local/lib:/home/my/CLAPACK-3.2.1=\n\"",
        "#",
        "$HADOOP_CONF_DIR,\n          $HADOOP_COMMON_HOME/share/hadoop/common/*,\n          $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,\n          $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,\n          $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,\n          $YARN_HOME/share/hadoop/mapreduce/*,\n          $YARN_HOME/share/hadoop/mapreduce/lib/*,\n          $YARN_HOME/share/hadoop/yarn/*,\n          $YARN_HOME/share/hadoop/yarn/lib/*",
        "$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-fr=\namework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/=\nhadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-=\nframework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/=\nyarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/h=\nadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/l=\nib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar=\n:/etc/hadoop/conf/secure",
        "$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framewo=\nrk/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoo=\np/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-frame=\nwork/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/=\nlib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop=\n/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:=\n/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc=\n/hadoop/conf/secure",
        "$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framewor=\nk/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop=\n/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framew=\nork/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/l=\nib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/=\nshare/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/=\nusr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/=\nhadoop/conf/secure",
        "$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/=\nhadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/c=\nommon/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framewor=\nk/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib=\n/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/sh=\nare/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/us=\nr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/ha=\ndoop/conf/secure",
        "${HADOOP_CONF_DIR}/mapred.include",
        "${HBASE.SERVER.ZOOKEEPER.QUORUM}",
        "${HOSTNAME}",
        "${LAUNCHER_POOL}",
        "${celer.hadoop.data.dir}",
        "${dfs.nameno=\nde.checkpoint.dir}",
        "${dfs.namenode.name.dir=\n}",
        "${dfs.namenode.name.dir}",
        "${fs.default.name}/hadoop_tmp/mapred/system",
        "${h=\nadoop.tmp.dir}/mapred/history/recoverystore",
        "${hadoop.tmp.d=\nir}/mapred/system",
        "${hadoop.tmp.dir=\n}/yarn-nm-recovery",
        "${hadoop.tmp.dir}/*nm*-local-*dir*",
        "${hadoop.tmp.dir}/=\nmapred/local",
        "${hadoop.tmp.dir}/=\nnm-local-dir",
        "${hadoop.tmp.dir}/dfs/data",
        "${hadoop.tmp.dir}/dfs/data/",
        "${hadoop.tmp.dir}/dfs/name",
        "${hadoop.tmp.dir}/dfs/name,/media/usb0/",
        "${hadoop.tmp.dir}/dfs/name/",
        "${hadoop.tmp.dir}/io/loca=\nl",
        "${hadoop.tmp.dir}/m=\napred/temp",
        "${hadoop.tmp.dir}/mapred/local",
        "${hadoop.tmp.dir}/mapred/staging",
        "${hadoop.tmp.dir}/mapred/system",
        "${hadoop.tmp.dir}/mapred/temp",
        "${hadoop.tmp.dir}/nm-local-dir",
        "${hbase.tmp.d=\nir}/zookeeper",
        "${hbase.tmp.dir}/local/",
        "${hbase.tmp.dir}/zookeeper",
        "${java.io.tmpdir}/hbase-${user.n=\name}",
        "${mapreduce.=\ntask.profile.params}",
        "${u=\nser.home}/hadoop.keytab",
        "${user.dir}/../logs",
        "${y=\narn.app.mapreduce.am.staging-dir}/history/done_intermediate",
        "${yarn.log.dir}/user=\nlogs",
        "${yarn.node=\nmanager.hostname}:8040",
        "${yarn.nodeman=\nager.hostname}:8042",
        "${yarn.nodemanager.ho=\nstname}:0",
        "${yarn.resourcema=\nnager.hostname}:8032",
        "${yarn.resourcemanager.hostname}:8030",
        "${yarn.resourcemanager.hostname}:8031",
        "${yarn.resourcemanager.hostname}:8032",
        "${yarn.resourcemanager.hostname}:8033",
        "${yarn.resourcemanager.hostname}:8088",
        "${yarn.timeline-=\nservice.hostname}:10200",
        "(&(&(objectclass=3Duser)(sAMAccountName=3D{0})))",
        "(objectclass=3Dgroup)",
        "*",
        "*\nrw",
        "* rw",
        "*0*",
        "*1*",
        "*127.0.0.1*",
        "*192.168.219.129*",
        "*30*",
        "*=\n",
        "*domU-12-31-39-00-E0-96.compute-1.internal*:60000",
        "*file:///temp/hbase-${user.name}/hbase*",
        "*hdfs://ec2-namdenode(master).*compute.amazonaws.com:8021",
        "*hdfs://localhost:9000*",
        "*localhost*",
        "*localhost:9001*",
        "*namenode.host.address*:50070",
        "*sha-cs-01,sha-cs-02,sha-cs-03,sha-cs-04,sha-=\ncs-06*",
        "*sha-cs-01,sha-cs-02,sha-cs-03,sha-cs-04,sha-cs-06*",
        "*smunnavar,test*",
        "-1",
        "-BLANKED",
        "-Djava.library.path=/usr/local/hadoop-2.4.0/lib/native",
        "-Dlog4j.configuration=/home/hanish/Desktop/log4j.properties",
        "-Dlog4j.configuration=/home/hanish/Desktop/log4j.properties\n-Xmx1024m -Dhadoop.root.logger=INFO,flume",
        "-XX:ErrorFile=/opt/cores/mapreduce_java_error%p.log",
        "-XX:ErrorFile=3D/opt/cores/mapreduce_java_error%p.log",
        "-Xms1024M -Xmx2048M",
        "-Xmx1000m",
        "-Xmx1024M",
        "-Xmx1024m",
        "-Xmx128m",
        "-Xmx1800m",
        "-Xmx180M",
        "-Xmx2000m",
        "-Xmx2048m",
        "-Xmx2560M",
        "-Xmx256m",
        "-Xmx384m",
        "-Xmx400m",
        "-Xmx4096m",
        "-Xmx40=\n96m",
        "-Xmx4g -Djava.awt.headless=true",
        "-Xmx500m",
        "-Xmx512m",
        "-Xmx5260m",
        "-Xmx768m",
        "-Xmx800m",
        "-agentlib:hprof=cpu=samples,heap=sites,depth=6,force=n,thread=y,verbose=n,file=%s",
        "-d Network",
        "-server -XX:NewRatio=3D8\n> -Djava.library.path=3D/usr/lib/hadoop/lib/native/\n> -Djava.net.preferIPv4Stack=3Dtrue",
        "-server -XX:NewRatio=3D8 =\n-Djava.library.path=3D/usr/lib/hadoop/lib/native/ =\n-Djava.net.preferIPv4Stack=3Dtrue",
        "./tmp",
        ".5",
        ".archive",
        "/\n> app/hadoop/tmp",
        "/DATA/zookeeper",
        "/SD1/hadoop_data",
        "/Users/ben//hbase",
        "/Users/ben/hbase/zookeeper",
        "/Users/jasonhuang/hdfs/data",
        "/Users/jasonhuang/hdfs/name",
        "/Users/me/deployments/current/data/hbase",
        "/Users/pat/zookeeper-data",
        "/Users/tynt/apps/hadoop/lib/native/Mac_OS_X-x86_64-64/",
        "/Users/tyu/tmp/zk",
        "/Volumes/Disk1/ZookeeperStorage",
        "/app-logs",
        "/app/hadoop/app",
        "/app/hadoop/dfs-2.2.0/data",
        "/app/hadoop/dfs-2.2.0/name",
        "/app/hadoop/tmp",
        "/app/hadoop/tmp/myoutput",
        "/app/software/app1",
        "/app/software/app=\n1",
        "/apps/hbase/staging",
        "/bgdt/hadoop/hdfs/jn",
        "/cygdrive/d/datarepublik/tmp/hadoop-${user.name\n> > }",
        "/cygdrive/d/datarepublik/tmp/hadoop-${user.name\n>>>>>>>>>>>                       \n>>> }",
        "/cygdrive/d/datarepublik/tmp/hadoop-${user.name}",
        "/da=\nta/1/dfs/nn,/nfsmount/dfs/nn",
        "/data",
        "/data/1/dfs/dn,/data/2/dfs/\n> dn,/data/3/dfs/dn",
        "/data/1/dfs/dn,/data/2/dfs/dn,/data/3/dfs/dn",
        "/data/1/dfs/jn",
        "/data/1/dfs/nn,/nfsmount/dfs/nn",
        "/data/1/dfs/snn,/nfsmount/dfs/snn",
        "/data/hadoop-0.20.0-${user.name}",
        "/data/hadoopdata",
        "/data/hadoopdataspace",
        "/data/hadoopnamespace",
        "/data/zookeeper",
        "/data0/hbase/filesystem/dfs/data,/data1/hbase/filesystem/dfs/data,/data2/hbase/filesystem/dfs/data,/data3/hbase/filesystem/dfs/data",
        "/data0/hbase/filesystem/dfs/data,/data1/hbase/filesystem/dfs/data,=\n/data2/hbase/filesystem/dfs/data,/data3/hbase/filesystem/dfs/data",
        "/data0/hbase/filesystem/dfs/name,/data1/hbase/filesystem/dfs/name,/data2/hbase/filesystem/dfs/name,/data3/hbase/filesystem/dfs/name",
        "/data0/hbase/filesystem/dfs/name,/data1/hbase/filesystem/dfs/name,=\n/data2/hbase/filesystem/dfs/name,/data3/hbase/filesystem/dfs/name",
        "/data0/hbase/filesystem/dfs/namesecondary,/data1/hbase/filesystem/=\ndfs/namesecondary,/data2/hbase/filesystem/dfs/namesecondary,/data3/hbase/fi=\nlesystem/dfs/namesecondary",
        "/data0/hbase/filesystem/dfs/namesecondary,/data1/hbase/filesystem/dfs/namesecondary,/data2/hbase/filesystem/dfs/namesecondary,/data3/hbase/filesystem/dfs/namesecondary",
        "/data0/hbase/filesystem/mapred/local,/data1/hbase/filesystem/mapre=\nd/local,/data2/hbase/filesystem/mapred/local,/data3/hbase/filesystem/mapred=\n/local",
        "/data0/hbase/filesystem/mapred/local,/data1/hbase/filesystem/mapred/local,/data2/hbase/filesystem/mapred/local,/data3/hbase/filesystem/mapred/local",
        "/data1,/data2,/data3,/data4",
        "/data1/hadoopfs,/data2/hadoopfs,/data3/hadoopfs,/data4/hadoopfs",
        "/data1/hbase/filesystem/mapred/system",
        "/data1/hbase/filesystem/tmp",
        "/data6/hdfs_ha1/ha_share1/share0/editlog",
        "/data6/hdfs_ha1/ha_share1/share0/namenode",
        "/data6/hdfs_ha1/ha_share1/share1/editlog",
        "/data6/hdfs_ha1/ha_share1/share1/namenode",
        "/deanhdfs/data/hadooptmp",
        "/dev/u=\nrandom",
        "/dev=\n/shm,/tmp",
        "/etc/hadoop/conf/allocations.xml",
        "/etc/hadoop/conf/hosts.exclude",
        "/etc/hadoop/conf/hosts.include",
        "/etc/hadoop/hdfs.keytab",
        "/etc/hadoop/keytab/hadoop.service.keytab",
        "/etc/security/keyt=\nab/jhs.service.keytab",
        "/export/home/hadoop/dfs/name",
        "/hadoop-2.6.0/etc/hadoop:/hadoop-2.6.0/share/hadoop/common/l=\nib/*:/hadoop-2.6.0/share/hadoop/common/*:/hadoop-2.6.0/share/hadoop/hdfs:/h=\nadoop-2.6.0/share/hadoop/hdfs/lib/*:/hadoop-2.6.0/share/hadoop/hdfs/*:/hado=\nop-2.6.0/share/hadoop/yarn/lib/*:/hadoop-2.6.0/share/hadoop/yarn/*:/hadoop-=\n2.6.0/share/hadoop/mapreduce/lib/*:/hadoop-2.6.0/share/hadoop/mapreduce/*:/=\nhadoop-2.6.0/contrib/capacity-scheduler/*.jar:/hadoop-2.6.0/share/hadoop/to=\nols/lib/*",
        "/hadoop-2.6.0/etc/hadoop:/hadoop-2.6.0/share/hadoop/common/lib/*:/=\nhadoop-2.6.0/share/hadoop/common/*:/hadoop-2.6.0/share/hadoop/hdfs:/hadoop-=\n2.6.0/share/hadoop/hdfs/lib/*:/hadoop-2.6.0/share/hadoop/hdfs/*:/hadoop-2.6=\n.0/share/hadoop/yarn/lib/*:/hadoop-2.6.0/share/hadoop/yarn/*:/hadoop-2.6.0/=\nshare/hadoop/mapreduce/lib/*:/hadoop-2.6.0/share/hadoop/mapreduce/*:/hadoop=\n-2.6.0/contrib/capacity-scheduler/*.jar:/hadoop-2.6.0/share/hadoop/tools/li=\nb/*",
        "/hadoop-2.6.0/etc/hadoop:/hadoop-2.6.0/share/hadoop/common/lib/*:/ha=\ndoop-2.6.0/share/hadoop/common/*:/hadoop-2.6.0/share/hadoop/hdfs:/hadoop-2.=\n6.0/share/hadoop/hdfs/lib/*:/hadoop-2.6.0/share/hadoop/hdfs/*:/hadoop-2.6.0=\n/share/hadoop/yarn/lib/*:/hadoop-2.6.0/share/hadoop/yarn/*:/hadoop-2.6.0/sh=\nare/hadoop/mapreduce/lib/*:/hadoop-2.6.0/share/hadoop/mapreduce/*:/hadoop-2=\n.6.0/contrib/capacity-scheduler/*.jar:/hadoop-2.6.0/share/hadoop/tools/lib/=\n*",
        "/hadoop-2.6.0/etc/hadoop:/hadoop-2.6.0/share/hadoop/common/lib/*:=\n/hadoop-2.6.0/share/hadoop/common/*:/hadoop-2.6.0/share/hadoop/hdfs:/hadoop=\n-2.6.0/share/hadoop/hdfs/lib/*:/hadoop-2.6.0/share/hadoop/hdfs/*:/hadoop-2.=\n6.0/share/hadoop/yarn/lib/*:/hadoop-2.6.0/share/hadoop/yarn/*:/hadoop-2.6.0=\n/share/hadoop/mapreduce/lib/*:/hadoop-2.6.0/share/hadoop/mapreduce/*:/hadoo=\np-2.6.0/contrib/capacity-scheduler/*.jar:/hadoop-2.6.0/share/hadoop/tools/l=\nib/*",
        "/hadoop/dfs/data/",
        "/hadoop/dfs/name,/hadoop/dfs/name2",
        "/hadoop/dfs/namesecondary/",
        "/hadoop/hdfs/data",
        "/hadoop/hdfs/name",
        "/hadoop/mapred/system",
        "/hadoop/security/conf/hadoop-http-auth-signature-secret",
        "/hadoop/security/keytab/hdfs.keytab",
        "/hadoop/tmp",
        "/hadoop/tmp/dfs/data,/opt/tmp/dfs/data",
        "/hadoop/tmp/log",
        "/hadoop/tmp/nm-local-dir",
        "/hadoop/zoo/data",
        "/hadoop/zookeeper/data",
        "/hbase",
        "/hbase-master",
        "/hbase-new",
        "/hbase-unsecure",
        "/hdfs-1/hadoop/hbase",
        "/hdfs/0/mapred/local,/hdfs/1/mapred/local,/hdfs/2/mapred/local,/=\nhdfs/3/mapred/local,/hdfs/4/mapred/local,/hdfs/5/mapred/local,/hdfs/6/mapre=\nd/local,/hdfs/7/mapred/local",
        "/hdfs/0/mapred/local,/hdfs/1/mapred/local,/hdfs/2/mapred/local,/h=\ndfs/3/mapred/local,/hdfs/4/mapred/local,/hdfs/5/mapred/local,/hdfs/6/mapred=\n/local,/hdfs/7/mapred/local",
        "/hdfs/0/mapred/local,/hdfs/1/mapred/local,/hdfs/2/mapred/local,/hdfs/3/mapred/local,/hdfs/4/mapred/local,/hdfs/5/mapred/local,/hdfs/6/mapred/local,/hdfs/7/mapred/local",
        "/hdfs/0/mapred/local,/hdfs/1/mapred/local,/hdfs/2/mapred/local,/hdfs=\n/3/mapred/local,/hdfs/4/mapred/local,/hdfs/5/mapred/local,/hdfs/6/mapred/lo=\ncal,/hdfs/7/mapred/local",
        "/hdfs/0/mapred/local,/hdfs/1/mapred/local,/hdfs/2/mapred/local,=\n/hdfs/3/mapred/local,/hdfs/4/mapred/local,/hdfs/5/mapred/local,=\n/hdfs/6/mapred/local,/hdfs/7/mapred/local",
        "/hdfs/zookeeper",
        "/hom=\ne/storage/namenode",
        "/home/${user.name}/dfs/data",
        "/home/${user.name}/dfs/name",
        "/home/${user.name}/mapred_scratch",
        "/home/${user.name}/tmp/hadoop",
        "/home/${user.name}/tmp_data",
        "/home/HadoopAdmin/hdfs/data",
        "/home/HadoopAdmin/hdfs/name",
        "/home/aim/tmp/hadoop-${user.name}",
        "/home/anand_vihar/hadoop-2.6.0/hdfs/data",
        "/home/anand_vihar/hadoop-2.6.0/hdfs/name",
        "/home/astie/hdfs/data",
        "/home/astie/hdfs/name",
        "/home/astie/hdfs/temp",
        "/home/bala/data",
        "/home/bala/name",
        "/home/beesh_hadoop2/zookeeper",
        "/home/beeshma/zookeeper",
        "/home/beeshma/zookeeper-3.4.6/conf",
        "/home/biginfolabs/BILSftwrs/hbase-0.94.10/hbstmp/",
        "/home/biginfolabs/BILSftwrs/hbase-0.94.10/zkptmp",
        "/home/cluster/Hadoop/hbase-0.90.4/temp",
        "/home/dan/zookeeper",
        "/home/data/1/yarn/local,/home/data/2/yarn/local,/home/data/3/yarn/lo\ncal",
        "/home/data/1/yarn/logs,/home/data/2/yarn/logs,/home/data/3/yarn/logs\n",
        "/home/data/tmp",
        "/home/e521596/hadoop-1.1.1/full",
        "/home/e521596/hadoop-1.1.1/full/dfs/data",
        "/home/e521596/hadoop-1.1.1/full/dfs/name",
        "/home/francis/.ssh/id_rsa",
        "/home/francis/hadoop-2.0.5-alpha/etc/hadoop/slaves",
        "/home/francis/hadoop2-hdfs/data",
        "/home/francis/hadoop2-hdfs/journalnode/data",
        "/home/francis/hadoop2-hdfs/name",
        "/home/francis/hadoop2-hdfs/tmp",
        "/home/francis/hadoop2-hdfs/yarn",
        "/home/francis/hadoop2-hdfs/yarn-log",
        "/home/hadoop-user/hadoop/conf/keytabs/http.keytab",
        "/home/hadoop/.ssh/id_rsa",
        "/home/hadoop/Desktop/hadoop-datastore/hadoop-$hadoop",
        "/home/hadoop/Desktop/hadoop-store/hadoop-$hadoop",
        "/home/hadoop/HbaseData/zookeeper",
        "/home/hadoop/Hbasetemp",
        "/home/hadoop/cdh4-dn-socket/dn=5Fsocket",
        "/home/hadoop/cdh4-dn-socket/dn_socket",
        "/home/hadoop/datadir2",
        "/home/hadoop/dfs,/tmp/hadoop/dfs",
        "/home/hadoop/dfs/namesecondary",
        "/home/hadoop/dfs16,/tmp/hadoop/dfs16",
        "/home/hadoop/dn",
        "/home/hadoop/hadoop-0.23.3/conf/security/username",
        "/home/hadoop/hadoop-2.0.3-alpha/etc/hadoop/topology.sh",
        "/home/hadoop/hadoop-data/namenode",
        "/home/hadoop/hadoop-data/namenode,/backup/hadoop/hadoop-data/nam=\nenode",
        "/home/hadoop/hadoop-data/namenode,/backup/hadoop/hadoop-data/namen=\node",
        "/home/hadoop/hadoop-data/namenode=2C/backup/hadoop/hadoop-data/nam=\nenode",
        "/home/hadoop/hadoop-dir/data-dir",
        "/home/hadoop/hadoop-dir/local-dir",
        "/home/hadoop/hadoop-dir/name-dir",
        "/home/hadoop/hadoop-dir/system-dir",
        "/home/hadoop/hadoop_tmp",
        "/home/hadoop/hadoop_tmp/dfs/data",
        "/home/hadoop/hadoop_tmp/dfs/name",
        "/home/hadoop/journal/data",
        "/home/hadoop/namedir2",
        "/home/hadoop/nn",
        "/home/hadoop/project/hadoop-data",
        "/home/hadoop/snn",
        "/home/hadoop/tmp",
        "/home/hadoop/tmp2",
        "/home/hadoop/zookeeper",
        "/home/hadoop/zookeeper_tmp",
        "/home/hadoop/zookeeperdata",
        "/home/hbase/hadoop/conf/dfs.hosts.exclude",
        "/home/hbase/hadoopdata/data/data",
        "/home/hbase/hadoopdata/mapred/local",
        "/home/hbase/hadoopdata/mapred/system",
        "/home/hbase/hadoopdata/name",
        "/home/hbase/hadoopdata/tmp",
        "/home/hbase/hadoopdata/zookeeper",
        "/home/hbase/hbase-data",
        "/home/hbase/zookeeper",
        "/home/hbaseuser/htrace.out",
        "/home/hduser/.ssh/id_rsa",
        "/home/hduser/hadoop/zookeeper",
        "/home/hduser/hadoop_ecosystem/apache_hbase/zk_datadir",
        "/home/hduser/hbase/local",
        "/home/hduser/hbase/tmp",
        "/home/hduser/hbase/zookeeper",
        "/home/hduser/mydata/hdfs/journalnode",
        "/home/huongntn/hadoop-${user.name}",
        "/home/imadas/24H-DB",
        "/home/imadas/zookeeperfolder",
        "/home/ivan/hadoop/hbase/conf/hbase.keytab",
        "/home/lab/hadoop-2.1.0-beta/tmp",
        "/home/libing/GreatFreeLabs/Hadoop/FS",
        "/home/lili/hbase-tmp",
        "/home/lili/zkdata",
        "/home/lili/zookeeperdata",
        "/home/liuxin/zookeeper/data",
        "/home/mallik/hadoop-${user.name}",
        "/home/mapcluster/hadoop_lat/hbase-0.90.4/temp",
        "/home/mapcluster/hadoop_lat/zookeeper",
        "/home/mapred/staging/done",
        "/home/mapred/staging/innerDone",
        "/home/morre/hbase/zookeeper",
        "/home/musa.ll/hdfs_ha/local/name",
        "/home/mustaqeem/development/hadoop-2.0.3-alpha/etc/hadoop/rack.sh",
        "/home/mustaqeem/development/hadoop-2.0.3-alpha/etc/hadoop=\n/rack.sh",
        "/home/mustaqeem/development/hadoop-2.0.3-alpha/etc=\n/hadoop/rack.sh",
        "/home/nodefence/.ssh/id_rsa",
        "/home/rahul/oodebesetup/data/hadoop/hdfs",
        "/home/runner/app/hadoop/dfs/data",
        "/home/runner/app/hadoop/tmp",
        "/home/runner/hbase/hbase-0.94.2/tmp",
        "/home/runner/hbase/hbase-0.94.=\n2/tmp",
        "/home/sas/hadoop-2.2.0/fs/datanode",
        "/home/sas/hadoop-2.2.0/fs/namenode",
        "/home/sas/hadoop-2.2.0/tmp",
        "/home/sas/hbase-0.98.0-hadoop2/hbase-${user.name}",
        "/home/shashwat/Hadoop/hbase-0.90.4/temp",
        "/home/shashwat/zookeeper",
        "/home/software/tmp/app-logs",
        "/home/software/tmp/node",
        "/home/steven/data/hbase-${user.name}",
        "/home/tianguanhua1/zookeeper/data",
        "/home/ubuntu/Desktop/hbase",
        "/home/ubuntu/Desktop/hbase/logs",
        "/home/ubuntu/Desktop/hbase/zookeeper",
        "/home/ubuntu/hdfs/tmp",
        "/home/user/space/hbase-${user.name}",
        "/home/vishnu/hadoop-tmp",
        "/home/wanli/hadoop/hbasetmp",
        "/home/xubuntu/Programs/hadoop/logs/history/done",
        "/home/xubuntu/Programs/hadoop/logs/history/intermediate-done-dir",
        "/home/yanglin/Programs/hadoop_tmp_dir/hadoop-${user.name}\n    ",
        "/home/yarn/local",
        "/home/yarn/log",
        "/home/yarn/logs",
        "/home/yarn/staging",
        "/home/zookeeper",
        "/home1/user1/dir1,/home2/user2/dir2",
        "/home1/user=\n1/dir1,/home2/user2/dir2",
        "/ht_data/hbase/zookeeper",
        "/jo=\nbtracker/jobsInfo",
        "/local/panton/hadoop-0.20.2-cdh3u0/dfs/zoo",
        "/localpath/hbase",
        "/logs",
        "/mapred/system",
        "/master",
        "/mnt",
        "/mnt/DP_disk1/hadoop2/hdfs/jn",
        "/mnt/disk1/hadoop=5Ftmp=5Fdir,/mnt/disk2/hadoop=5Ftmp=5Fdir,=\n/mnt/disk3/hadoop=5Ftmp=5Fdir",
        "/mnt/disk1/hadoop_tmp_dir,/mnt/disk2/hadoop_tmp_dir,/mnt/disk3/h=\nadoop_tmp_dir",
        "/mnt/disk1/hadoop_tmp_dir,/mnt/disk2/hadoop_tmp_dir,/mnt/disk3/had=\noop_tmp_dir",
        "/mnt/disk1/hadoop_tmp_dir,/mnt/disk2/hadoop_tmp_dir,/mnt/disk3/hadoo=\np_tmp_dir",
        "/mnt/disk1/hadoop_tmp_dir,/mnt/disk2/hadoop_tmp_dir,/mnt/disk3=\n/hadoop_tmp_dir",
        "/mnt/ext/hadoop/hdfs/namenode",
        "/mnt/hadoop",
        "/mnt/hadoop/data",
        "/mnt/hdfs/ebs1,/mnt/hdfs/ebs2,/mnt/hdfs/ebs3,/mnt/hdfs/ebs4,/mn=\nt/hdfs/ebs5,/mnt/hdfs/ebs6",
        "/mnt/hdfs/ebs1,/mnt/hdfs/ebs2,/mnt/hdfs/ebs3,/mnt/hdfs/ebs4,/mnt/hdfs/ebs5,/mnt/hdfs/ebs6",
        "/mnt/san1/hdfs/${user.name}/dfs/data,/mnt/san2/hdfs/${user.name\n}/dfs/data,/mnt/san3/hdfs/${user.name}/dfs/data,/mnt/san4/hdfs/${user.name\n}/dfs/data",
        "/mnt/zookeeper",
        "/mr-history/done",
        "/mr-history/tmp",
        "/my/own/external/jars",
        "/my/path/hbase-data/zookeeper",
        "/mybk/zookeeper",
        "/myhadoop/etc/hadoop/datanode-excludes",
        "/myhadoop/etc/hadoop/nodemanager-excludes",
        "/opt/data/hadoop",
        "/opt/data/zookeeper",
        "/opt/hadoop-2.0.4-alpha/temp/hadoop/dfs_data_dir",
        "/opt/hadoop-2.0.4-alpha/temp/hadoop/dfs_name_dir",
        "/opt/hadoop-2.0.4-alpha/temp/hadoop/mapreduce_local_dir",
        "/opt/hadoop-2.0.4-alpha/temp/hadoop/mapreduce_temp",
        "/opt/hadoop-2.0.4-alpha/temp/hadoop/yarn_nm_app-logs",
        "/opt/hadoop-2.0.4-alpha/temp/hadoop/yarn_nm_local_dir",
        "/opt/hadoop-2.0.4-alpha/temp/hadoop/yarn_nm_log",
        "/opt/hadoop/hadoop/tmp",
        "/opt/hadoop/hdfs/data",
        "/opt/hadoop/hdfs/name",
        "/opt/hadoop/tmp",
        "/opt/hbase/zookeeper",
        "/opt/logs/hadoop/history",
        "/opt/yarn/hadoop-2.3.0/etc/hadoop,/opt/yarn/hadoop-2.3.0/*,/opt/=\nyarn/hadoop-2.3.0/lib/*,/opt/yarn/hadoop-2.3.0/*,/opt/yarn/hadoop-2.3.0/lib=\n/*,/opt/yarn/hadoop-2.3.0/*,/opt/yarn/hadoop-2.3.0/lib/*,/opt/yarn/hadoop-2=\n.3.0/*,/opt/yarn/hadoop-2.3.0/lib/*",
        "/path/to/exclude_dn_list",
        "/path/to/hdfs/nodemanager_log/",
        "/path/to/hivetmp/dir/on/local/linux/disk",
        "/path/to/keytab/hbase.keytab",
        "/path/to/topo.sh",
        "/proj/ucare/bo/hadoop_data/zookeeper",
        "/root/.ssh/id_rsa",
        "/root/Programs/hadoop/etc/hadoop/dfs.include",
        "/root/hive/bui=\nld/dist/lib/hive-hbase-handler-0.9.0-amplab-4.jar,/root/hive/build/dist/lib=\n/hbase-0.92.0.jar,/root/hive/build/dist/lib/zookeeper-3.4.3.jar,/root/hive/=\nbuild/dist/lib/guava-r09.jar",
        "/root/hive/build/dist/lib/hive-hbase-handler-0.9.0-amplab-4.jar,/root/hive/build/dist/lib/hbase-0.92.0.jar,/root/hive/build/dist/lib/zookeeper-3.4.3.jar,/root/hive/build/dist/lib/guava-r09.jar",
        "/scratch/software/hadoop2/hadoop-dc/node_health.sh",
        "/scratch/ybedekar/hbase0.94.8logs/zookeeper",
        "/software/home/hadoop/hadoop/hdfstmp",
        "/somedirectory",
        "/srv/cloud/hadoop/cache/hadoop/mapred",
        "/ssd/hbase/hbase-0.94.16/zookeeper",
        "/state/partition1/hadoop/dfs",
        "/state/partition1/hadoop/dfs16",
        "/storage/disk1/hbase.local.dir,/storage/disk2/hbase.local.dir,/sto=\nrage/disk3/hbase.local.dir",
        "/storage/disk1/hbase.local.dir,/storage/disk2/hbase.local.dir,/storage/disk3/hbase.local.dir",
        "/tmp",
        "/tmp/${user.name}",
        "/tmp/.hdfs-nfs",
        "/tmp/data/dfs/data/",
        "/tmp/data/dfs/name/",
        "/tmp/hadoop",
        "/tmp/hadoop-${user.name}",
        "/tmp/hadoop-=\nyarn/staging",
        "/tmp/hadoop-bala/dfs/data",
        "/tmp/hadoop-bala/dfs/name",
        "/tmp/hadoop-temp",
        "/tmp/hbase",
        "/tmp/hbase-${user.name}-dev",
        "/tmp/hbase-${user.name}-xtest",
        "/tmp/hbase-${user.name}/zk",
        "/tmp/hbase-sao_user/zookeeper",
        "/tmp/local",
        "/tmp/mapred",
        "/tmp/mapred/system/",
        "/tmp/mapred_tmp",
        "/tmp/zookeeper_data",
        "/u0/journal/node/local/data",
        "/user",
        "/user/hive/warehouse",
        "/usr/bin/health_checker",
        "/usr/bin/kinit",
        "/usr/hadoop/hdfs/data",
        "/usr/hadoop/hdfs/name",
        "/usr/hadoop/tmp/dfs/data, /dev/vdb ",
        "/usr/hadoop/tmp/dfs/data,/sda",
        "/usr/lib/gphd/hadoop/*,/usr/lib/gphd/hadoop/lib/*,/usr/lib/gphd/ha=\ndoop-hdfs/*,/usr/lib/gphd/hadoop-hdfs/lib/*,/usr/lib/gphd/hadoop-yarn/*,/us=\nr/lib/gphd/hadoop-yarn/lib/*,/usr/lib/gphd/hadoop-mapreduce/*,/usr/lib/gphd=\n/hadoop-mapreduce/lib/*,/etc/gphd/hadoop/conf/*",
        "/usr/lib/gphd/hadoop/*,/usr/lib/gphd/hadoop/lib/*,/usr/lib/gphd/had=\noop-hdfs/*,/usr/lib/gphd/hadoop-hdfs/lib/*,/usr/lib/gphd/hadoop-yarn/*,/usr/=\nlib/gphd/hadoop-yarn/lib/*,/usr/lib/gphd/hadoop-mapreduce/*,/usr/lib/gphd/ha=\ndoop-mapreduce/lib/*,/etc/gphd/hadoop/conf/*",
        "/usr/lib/gphd/hadoop/*,/usr/lib/gphd/hadoop/lib/*,/usr/lib/gphd/hadoo=\np-hdfs/*,/usr/lib/gphd/hadoop-hdfs/lib/*,/usr/lib/gphd/hadoop-yarn/*,/usr/li=\nb/gphd/hadoop-yarn/lib/*,/usr/lib/gphd/hadoop-mapreduce/*,/usr/lib/gphd/hado=\nop-mapreduce/lib/*,/etc/gphd/hadoop/conf/*",
        "/usr/local/hadoop/conf/dfs_exclude",
        "/usr/local/hadoop/conf/exclude",
        "/usr/local/hadoop/conf/include",
        "/usr/local/hadoop/conf/mapred.include",
        "/usr/local/hadoop/datastore/hadoop-${user.name}",
        "/usr/local/hadoop/includehosts",
        "/usr/local/hadoop/tmp",
        "/usr/local/hbase/data",
        "/usr/local/hbase/hbase-0.94.2/data/zookeeper",
        "/usr/local/hbase/zookeeperdata",
        "/usr/local/hive/mydir",
        "/usr/xxx/hdfs",
        "/usr/xxx/hdfs/data",
        "/usr/xxx/hdfs/hbase",
        "/usr/xxx/hdfs/name",
        "/var/cache/hadoop-hdfs/hdfs",
        "/var/hadoop/${user.name}",
        "/var/hadoop/tmp",
        "/var/hbase",
        "/var/hbase-hadoop/data",
        "/var/hbase-hadoop/name",
        "/var/hbase-hadoop/namesecondary",
        "/var/hbase-hadoop/tmp",
        "/var/hbase-hadoop/zookeeper",
        "/var/hdfs-data",
        "/var/hdfs-data-name",
        "/var/lib/hadoop-0.20/cache/hadoop/dfs/name",
        "/var/lib/hadoop-hdfs/cache/${user.name\n> }/dfs/namesecondary",
        "/var/lib/hadoop-hdfs/cache/${user.name\n>> }/dfs/namesecondary",
        "/var/lib/hadoop-hdfs/cache/${user.name\n}/dfs/namesecondary",
        "/var/lib/hadoop-hdfs/cache/${user.name}",
        "/var/lib/hadoop-hdfs/cache/${user.name}/dfs/name",
        "/var/lib/hadoop-hdfs/cache/${user.name}/dfs/namesecondary",
        "/var/lib/hadoop-hdfs/cache/hdfs/dfs/name",
        "/var/lib/hadoop/data1,/var/lib/hadoop/data2",
        "/var/lib/hadoop/dn_socket",
        "/var/lib/hbase/cache/${user.name}/root",
        "/var/lib/hbase/cache/${user.name}/tmp",
        "/var/log/hadoop-yarn/apps",
        "/var/log/hadoop/yarn",
        "/var/mapred_local/*HOSTNAME*/",
        "/var/run/hadoop-hdfs/dn._PORT",
        "/var/run/hdfs-sockets/d=\nn",
        "/var/zookeeper",
        "/vol/persistent-hdfs",
        "/wsadfs/${host.name}/name",
        "/zookeeper_data",
        "0",
        "0-2",
        "0-5",
        "0.0",
        "0.0.0.0",
        "0.0.0.0:0",
        "0.0.0.0:10016",
        "0.0.0.0:10020",
        "0.0.0.0:100=\n33",
        "0.0.0.0:11010",
        "0.0.0.0:18050",
        "0.0.0.0:18088",
        "0.0.0.0:19888",
        "0.0.0.0:19=\n888",
        "0.0.0.0:50010",
        "0.0.0.0:50011",
        "0.0.0.0:50020",
        "0.0.0.0:50030",
        "0.0.0.0:5003=\n0",
        "0.0.0.0:50070",
        "0.0.0.0:50075",
        "0.0.0.0:50090",
        "0.0.0.0:500=\n60",
        "0.0.0.0:50105=\n",
        "0.0.0.0:50472",
        "0.0.0.0:50477",
        "0.0.0.0:50=\n090",
        "0.0.0.0:51100",
        "0.0.0.0:51110",
        "0.0.0.0:51130",
        "0.0.0.0:51160",
        "0.0.0.0:51170",
        "0.0.0.0:51175",
        "0.0.0.0:5=\n0091",
        "0.0.0.0:60000",
        "0.0.0.0:60020",
        "0.0.0.0:62020",
        "0.0.0.0:8030",
        "0.0.0.0:9003",
        "0.0.0.0:9290",
        "0.0.0.0=\n:50070",
        "0.05",
        "0.1",
        "0.15",
        "0.2",
        "0.207",
        "0.25",
        "0.3",
        "0.35",
        "0.38",
        "0.3f",
        "0.4",
        "0.40",
        "0.45",
        "0.48",
        "0.5",
        "0.58",
        "0.5f",
        "0.6",
        "0.75",
        "0.75f",
        "0.8",
        "0.9",
        "0.94.8-SNAPSHOT",
        "0.95-SNAPSHOT",
        "0.9f",
        "000",
        "022",
        "0=\n",
        "0=\n.05",
        "1",
        "1.0",
        "1.=\n0",
        "10",
        "10.0.0.1",
        "10.0.0.1:54311",
        "10.0.0.30",
        "10.0.10.1:50070",
        "10.0.10.1:8020",
        "10.0.10.2:50070",
        "10.0.10.2:8020",
        "10.0.2.31:50070",
        "10.0.2.31:8020",
        "10.0.2.32:50070",
        "10.0.2.32:8020",
        "10.0.40.1",
        "10.1.1.2:60000",
        "10.10.10.1",
        "10.10.113.6:60000",
        "10.101.89.68:50070",
        "10.101.89.68:50090",
        "10.101.89.68:9001",
        "10.101.89.69:50070",
        "10.101.89.69:50090",
        "10.101.89.69:9001",
        "10.108.99.68:50070",
        "10.108.99.68:50090",
        "10.108.99.68:9001",
        "10.108.99.69:50070",
        "10.108.99.69:50090",
        "10.108.99.69:9001",
        "10.114.45.186",
        "10.14.24.19",
        "10.2.44.131,10.2.44.132",
        "10.211.55.7:60000",
        "10.232.98.77:20020",
        "10.232.98.78:20020",
        "10.232.98.79:20020",
        "10.251.27.130,10.250.9.220,10.251.110.50",
        "10.255.1.61",
        "10.42.253.182:60000",
        "10.50.0.10:50010",
        "10.50.0.10:50075",
        "100",
        "1000",
        "10000",
        "100000",
        "1000000",
        "10000000",
        "1000000000",
        "10001",
        "1000=\n",
        "100663296",
        "1023",
        "1024",
        "10240",
        "102400",
        "1024768",
        "102476=\n8",
        "1048576",
        "10485760",
        "104857600",
        "104=\n8576",
        "10705960960",
        "1073741824",
        "10737418240",
        "107374182400",
        "10737418240000",
        "10800",
        "10=\n0",
        "10=\n00",
        "112",
        "12",
        "12.34.56.78",
        "120",
        "12000",
        "120000",
        "1200000",
        "12047",
        "1209600",
        "123",
        "1234",
        "12600000",
        "127.0.0.1",
        "127.0.0.1:0",
        "127.0.0.1:54311",
        "127.0.0.1:59000",
        "127.0.0.1:60000",
        "127.0.0.1:8021",
        "127.0.0.1=\n:0",
        "128",
        "13",
        "131072",
        "132.148.0.10:9001",
        "132.148.0.11:9001",
        "132.168.0.10",
        "134217728",
        "134217728=\n",
        "13562",
        "137.195.143.129:10020",
        "137.195.143.129:19888",
        "14",
        "15",
        "150",
        "15040",
        "15050",
        "1536",
        "16",
        "160.110.185.93,160.110.79.33,160.110.79.60,160.110.185.160,160.110.185.183",
        "160.110.185.93,160.110.79.33,160.110.79.60,160.110.185.160,160.110.1=\n85.183",
        "160.110.185.93,160.110.79.33,160.110.79.60,160.110.185.160,160.110=\n.185.183",
        "161061273600",
        "16384",
        "16777216",
        "172.20.189.44",
        "172.20.8.207,172.20.8.90,172.20.8.56",
        "172.23.34.12,172.23.34.80,172.23.34.85",
        "172.28.1.138,172.28.2.136",
        "172.28.1.138:60000",
        "172.31.20.187:8021",
        "172.31.20.187:8030",
        "172.31.20.187:8031",
        "172.31.20.187:8032",
        "172.31.25.237:10020",
        "172.31.3.150",
        "172.31.3.150:8021",
        "18",
        "180",
        "1800",
        "180000",
        "1800000",
        "18000000",
        "18000=\n0",
        "192.168.0.0:50010",
        "192.168.0.0:50020",
        "192.168.0.0:50070",
        "192.168.0.0:50075",
        "192.168.0.0:50090",
        "192.168.0.101:9001",
        "192.168.0.153 ro",
        "192.168.0.2:60000",
        "192.168.1.101:60000",
        "192.168.1.122",
        "192.168.1.1:50001",
        "192.168.1.5:60000",
        "192.168.1.68",
        "192.168.10.13",
        "192.168.10.22",
        "192.168.10.22,192.168.10.34,192.168.10.35",
        "192.168.10.22:2281,192.168.10.34:2281,192.168.10.35:2281",
        "192.168.10.22:50070",
        "192.168.10.23,192.168.10.24,192.168.10.49",
        "192.168.10.49:10020",
        "192.168.10.49:19888",
        "192.168.107.142,192.168.107.149,192.168.106.202",
        "192.168.11.151,192.168.11.152,192.168.11.153,192.168.11.154,192.16=\n8.11.155,192.168.11.156,192.168.11.157,192.168.11.158",
        "192.168.2.3",
        "192.168.2.3:60000",
        "192.168.20.30",
        "192.168.219.129:10020",
        "192.168.219.129:19888",
        "192.168.219.129:50070",
        "192.168.219.129:8020",
        "192.168.219.129:8030",
        "192.168.219.129:8031",
        "192.168.219.129:8033",
        "192.168.219.129:8034",
        "192.168.219.129:8088",
        "192.168.219.129:9001",
        "192.168.219.132:50070",
        "192.168.219.132:8020",
        "192.168.25.18, 192.168.25.19",
        "192.168.33.204:11005",
        "192.168.33.204:62000",
        "192.168=\n.1.68",
        "192.=\n168.1.68",
        "1=\n",
        "1=\n073741824",
        "1=\n2",
        "2",
        "2.1",
        "2.4",
        "20",
        "200",
        "2000",
        "20000",
        "2000000",
        "200=\n000",
        "2047",
        "2048",
        "2048000000",
        "2049",
        "205.172.170.25:8025",
        "205.172.170.25:8030",
        "205.172.170.25:8040",
        "205.172.170.25:9046",
        "206.88.43.8,206.88.43.4",
        "206.88.43.8:60000",
        "2097152",
        "20971520",
        "209715200",
        "2097152=\n",
        "21",
        "2147483647",
        "2147483647=\n",
        "2147483648",
        "2181",
        "21810",
        "2182",
        "2183",
        "22000",
        "2222",
        "2281",
        "24",
        "240000",
        "2400000",
        "24560",
        "24561",
        "24562",
        "24563",
        "24567",
        "24568",
        "24570",
        "24571",
        "25",
        "256",
        "256435456",
        "25=\n5",
        "268435456",
        "2684354560",
        "2888",
        "2889",
        "2890",
        "3",
        "30",
        "300",
        "3000",
        "30000",
        "300000",
        "3000000",
        "30000000",
        "3072",
        "3145728",
        "314=\n5728",
        "3181",
        "32",
        "32000000",
        "32=\n",
        "33554432",
        "3500",
        "3600",
        "36000",
        "360000",
        "3600000",
        "36000000",
        "3600=\n0",
        "3888",
        "3889",
        "3890",
        "3=\n",
        "4",
        "40",
        "4000",
        "40000",
        "400000",
        "4000000000",
        "4096",
        "4194304",
        "4242",
        "43200000",
        "45000",
        "456",
        "5",
        "50",
        "500",
        "5000",
        "500000000",
        "50070",
        "500=\n0",
        "50100-50200",
        "50470",
        "5073741824",
        "5073741824\n> >>> ",
        "5073741824\n> >>>>> ",
        "5073741824\n> >>>>>>>>> ",
        "5073741824\n> >>>>>>>>>>> ",
        "5073741824\n> >>>>>>>>>>>>> ",
        "5073741824\n>> ",
        "5073741824\n>>> ",
        "5073741824\n>>>>> ",
        "5073741824\n>>>>>>> ",
        "5073741824\n>>>>>>>>> ",
        "5073741824\n>>>>>>>>>>> ",
        "5073741824\n>>>>>>>>>>>>> ",
        "507374182=\n4\n> >>>>>>> ",
        "5096",
        "5097152",
        "512",
        "5120",
        "5260",
        "536870912",
        "5368709120",
        "58.155.50.240,58.155.50.228,58.155.50.224",
        "58.155.50.240:60000",
        "58080",
        "5=\n",
        "6",
        "60",
        "600",
        "6000",
        "60000",
        "600000",
        "60001",
        "6000=\n00",
        "60010",
        "60011",
        "60020",
        "60030",
        "60031",
        "60080",
        "6010",
        "604800",
        "604800000",
        "604=\n800000",
        "60=\n000",
        "60=\n0000",
        "61000",
        "61010",
        "61020",
        "61030",
        "6144",
        "62010",
        "62030",
        "62181",
        "6250000",
        "6291456",
        "63888",
        "64",
        "640",
        "64m",
        "6500",
        "65536",
        "67108864",
        "6=\n0000",
        "7",
        "70",
        "700",
        "72",
        "720000",
        "72800",
        "755",
        "8",
        "80",
        "800000",
        "8022",
        "8023",
        "8024",
        "8025",
        "8080",
        "8192",
        "8196",
        "8388608",
        "84000",
        "86400000",
        "86400000000",
        "86400=\n000",
        "86=\n400000",
        "88",
        "8=\n6400000",
        "9",
        "9000",
        "90000",
        "900000",
        "90000000",
        "90000=\n",
        "90000=\n0",
        "9000=\n",
        "9000=\n0",
        "90=\n0000",
        "9216,17408,33792,66560",
        "96",
        "9=\n000",
        "=\n/data/1/dfs/nn,/nfsmount/dfs/nn",
        "=\n0",
        "=\n0.25",
        "=\n0.38",
        "=\n0.4",
        "=\n0.70",
        "=\n1",
        "=\n1000",
        "=\n3600000",
        "=\n4",
        "=\n600000",
        "=\nHTTP/_HOST@LOCALHOST",
        "=\nfalse",
        "=\ntrue",
        "A:9001",
        "AES/CTR/No=\nPadding",
        "AWS_ACCESS_KEY_ID",
        "AWS_SECRET_ACCESS_KEY",
        "AutoReduce",
        "BLOCK",
        "C:/cygwin/dfs/data",
        "C:/cygwin/dfs/logs",
        "C:\\Desarrollo\\hbase-0.98.7-hadoop2\\data\\zookeeper",
        "CH22:8088",
        "DC-TEST-1",
        "DEBUG",
        "DEFAULT",
        "DEFAULT_COMPRESSION",
        "DESKTOP1:10020",
        "DESKTOP1:19888",
        "HADOOP/HBASE-0.92.1/hbase-0.92.1-security",
        "HTTP/_HOST@HADOOP.COM",
        "HTTP/_HOST@REALM",
        "HTTP/_HOST@RND.HDFS.COM",
        "HTTP/hpctest3.realm.com@REALM.COM",
        "HTTP/master.hadoop.local@HADOOP.LRZ.DE",
        "HTTPS_ONLY",
        "HTTP_ONLY",
        "Hadoop01:8040",
        "Hadoopserver,Hadoopclient1,Hadoopclient",
        "INFO",
        "INFO,flume\n-Dlog4j.configuration=/home/hanish/Desktop/log4j.properties",
        "INFO,flume\n-Dlog4j.configuration=/home/hanish/Desktop/log4j.properties\n-Dhadoop.root.logger=INFO,flume",
        "JAVA_LIBRARY_PATH=3D/home/hdfs/hadoop-0.20.2-CDH3B4/lib/native/Linux=\n-amd64-64",
        "JAVA_LIBRARY_PATH=3D/usr/lib/hadoop-2.0.1-alpha/lib/native/Linux-a=\nmd64-64",
        "LD_LIBRARY_PATH=$HADOOP_HOME/lib/native",
        "LD_LIBRARY_PATH=3D$HADOOP_COMMON_HOME/lib/native:/opt/snappy/lib\n:$LD_LIBRARY_PATH",
        "LD_LIBRARY_PATH=3D$HADOOP_COMMON_HOME/lib/native:/opt/snappy/lib=\n:\n$LD_LIBRARY_PATH",
        "MACHINENAME:60000",
        "MACHINENAME=\n:60000",
        "MALLOC_ARENA_MAX=3D$MALLOC_ARENA_MAX,LD_LIBRARY_PATH=3D$HADOOP_CO=\nMMON_HOME/lib/native:/\nopt/snappy/lib:$LD_LIBRARY_PATH",
        "MALLOC_ARENA_MAX=3D=\n$MALLOC_ARENA_MAX",
        "MyMasterServerName",
        "NONE",
        "NameNode:54311",
        "None",
        "RECORD",
        "RULE:[1:$1@$0](.*@\\Q\\E$)s/@\\Q\\E$//\n>\n> RULE:[2:$1@$0](.*@\\Q\\E$)s/@\\Q\\E$//\n>\n> DEFAULT",
        "RULE:[1:$1@$0](.*@\\Q\\E$)s/@\\Q\\E$//\n>>\n>> RULE:[2:$1@$0](.*@\\Q\\E$)s/@\\Q\\E$//\n>>\n>> DEFAULT",
        "RUNNING",
        "RhCluster",
        "RoundRobinPool",
        "S=\nHA1PRNG",
        "VM_11",
        "YOUR_HOST:10020",
        "YOUR_HOST:19888",
        "ZH-HOST_MACHINE",
        "ZK_HOSTS",
        "^[A-Za-=\nz_][A-Za-z0-9._-]*[$]?$",
        "a,b",
        "a,b,c",
        "abcd",
        "abcde.yyyy.com:9921",
        "acl",
        "admin",
        "akhettar",
        "all",
        "app/zookeeper",
        "atlas:60000",
        "authentication",
        "baby20:10020",
        "baby20:19888",
        "baby20:8025",
        "baby20:8030",
        "baby20:8040",
        "baby20:8088",
        "baby20:8141",
        "bgdt-dev-hrb",
        "bgdt01.dev.hrb:50070",
        "bgdt01.dev.hrb:50470",
        "bgdt01.dev.hrb:9000",
        "bgdt02.dev.hrb:50070",
        "bgdt02.dev.hrb:50470",
        "bgdt02.dev.hrb:9000",
        "bgdtgrp",
        "bin/cpc",
        "blade1,blade2,blade3",
        "blade1,blade2,blade4",
        "blade1:60000",
        "blr2211457.idc.oracle.com",
        "blr2211457.idc.oracle.com:60000",
        "box1,box2,box3,box4",
        "c:\\\\wksp",
        "chanel2,crunch2,crunch3,chris,chanel",
        "classic",
        "cldx-1140-1034",
        "clone11",
        "clone1:50070",
        "clone1:8020",
        "clone2:50070",
        "clone2:8020",
        "cluster",
        "clusterAnn1:8000",
        "clustera, clusterb",
        "col_fam:name",
        "com.bil.coproc.ColumnAggregationEndpoint",
        "com.dianping.h=\nbase.ClientGetRegionObserver",
        "com.dianping.hbase.ClientGetRegionObserver",
        "com.hadoop.compression.lzo.LzoCodec",
        "com.hbase.example.region.coprocessors.MyCustomRegionObserver",
        "com.myemployer.group.project.hbase.monitoring.coprocessor",
        "com.mypack.MyAuxServiceClassForBar",
        "com.mypack.MyAuxServiceClassForFoo",
        "com.mysql.jdbc.Driver",
        "com.ngdata.sep.impl.SepReplicationSource",
        "com.quobyte.hadoop.QuobyteFileSystem",
        "com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImp=\nl",
        "com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl",
        "com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl=\n",
        "conf/rack-awareness.sh",
        "convera",
        "cp classPath....",
        "cube",
        "cube,latitude,node3",
        "dc",
        "debian",
        "default",
        "default,small",
        "dev32",
        "devrackA-00,devrackA-01,devrackA-25",
        "dfs/data",
        "dfs/name",
        "domU-12-31-39-00-13-A1.compute-1.internal:50002",
        "domU-12-31-39-00-13-A1.compute-1.internal:60000",
        "domU-12-31-39-00-25-C1.compute-1.internal:50002",
        "domU-12-31-39-00-25-C1.compute-1.internal:60000",
        "domU-12-31-39-00-E5-D2.compute-1.intern=\nal:60000",
        "domU-12-31-39-00-E5-D2.compute-1.internal:50002",
        "domU-12-31-39-00-E5-D2.compute-1.internal:60000",
        "domU-12-31-39-06-29-91.compute-1.internal",
        "domU-15-35-19-0B-DD-29.compute-1.internal",
        "domU-15-35-19-0B-DD-29.compute-1.internal:60000",
        "domU-1=\n2-31-39-00-E5-D2.compute-1.internal:50002",
        "dr.=\nwho=3D;",
        "dynobuntu17",
        "ec2-184-73-22-146.compute-1.amazonaws.com:9001",
        "ec2-67-202-24-167.compute-1.amazonaws.com:60000",
        "ec2-67-202-57-127.compute-1.amazonaws.com:60020",
        "ec2-67-202-57-127.compute-1.amazonaws.com:60020\n> ",
        "ec2-67-202-57-127.compute-1.amazonaws.com:60020\n> > ",
        "ec2-67-202-57-127.compute-1.amazonaws.com:60020\n> > > ",
        "ec2externaldns:60020",
        "elroy:6000",
        "emr.hbase.fs.BlockableFileSyst=\nem",
        "emr.hbase.fs.BlockableFileSyste=\nm",
        "emr.hbase.fs.BlockableFileSystem=\n",
        "eth0",
        "eth1",
        "f=\nalse",
        "fal=\nse",
        "fals=\ne",
        "false",
        "false=\n",
        "fbase.cinchcorp.com:60020",
        "fedora3:9001",
        "file://${hadoop.tmp.dir}=\n/dfs/data",
        "file:///",
        "file:////Users/tyu/tmp",
        "file:///C:/Desarrollo/hbase-0.98.7-hadoop2/data/hbase",
        "file:///Users/${user.name}/hbase-data/dev",
        "file:///Users/${user.name}/hbase-data/xtest",
        "file:///Users/pat/hbase-data",
        "file:///app/hbase",
        "file:///bgdt/hadoop/hdfs/dn",
        "file:///bgdt/hadoop/hdfs/nn",
        "file:///data/hbase",
        "file:///data/namespace/1,file:///data/namespace/2",
        "file:///data1/hadoop/hdfs/namenode",
        "file:///data1/hdfs/data,file:///data2/hdfs/data,file:///data3/hdfs/data,file:///data4/hdfs/data",
        "file:///dfs/nn,file:///d=\nata2/dfs/nn",
        "file:///hadoop/hbase/hbase-0.98.11-hadoop2/data/",
        "file:///hadoop/hbase/hbase-0.98.11-hadoop2/data/zookeeper",
        "file:///hadoop_temp",
        "file:///hbase_data",
        "file:///hdfs/name,file:///export/hdfs/name",
        "file:///hdfs/namesecondary",
        "file:///home/beesh_hadoop2/hbase",
        "file:///home/dan/hbase",
        "file:///home/hadoopadmin",
        "file:///home/hduser/hdfs/datanode",
        "file:///home/hduser/hdfs/namenode",
        "file:///home/lili/hbasedata",
        "file:///home/srikanth/hbase-store",
        "file:///home/yogesh/hbase.dir",
        "file:///ht_data/hbase/hbase",
        "file:///lustre/site-h/tmp/susan",
        "file:///lustre/site-h/tmp/susan/data1/hdfs/data,file:///lust=\nre/site-h/tmp/susan/data2/hdfs/data,file:///lustre/site-h/tmp/susan/data3/h=\ndfs/data",
        "file:///mnt/nfs_disk/hbase/",
        "file:///pic/scratch/rtaylor/hbase",
        "file:///scratch/mingtzha/hbase/test",
        "file:///scratch/ybedekar/hbase0.94.8logs/hbase",
        "file:///temp/hbase-${user.name}/hbase",
        "file:///tmp/hbase",
        "file:///tmp/hbase-${user.name}",
        "file:///usr/lib/hbase-0.92.0/data",
        "file:///usr/local/hadoop/hadoop_data/hdfs/namenode",
        "file:///usr/local/hadoop/hadoop_store/hdfs/datanode",
        "file:///usr/local/hadoop/local",
        "file:///usr/local/hbase",
        "file:///usr/local/hive/lib/zookeeper-3.4.5.jar,file:/usr/local/hiv=\ne/lib/hive-hbase-handler-0.13.1.jar,file:///usr/local/hive/lib/guava-11.0.2=\n.jar,file:///usr/local/hbase/lib/hbase-client-0.98.2-\n> hadoop2.jar,file:///usr/local/hbase/lib/hbase-common-0.98.2-hadoop2.jar,f=\nile:///usr/local/hbase/lib/hbase-protocol-0.98.2-hadoop2.jar,file:///usr/lo=\ncal/hbase/lib/hbase-server-0.98.2-hadoop2.jar,file:///usr\n> /local/hbase/lib/hbase-shell-0.98.2-hadoop2.jar,file:///usr/local/hbase/l=\nib/hbase-thrift-0.98.2-hadoop2.jar",
        "file:///usr/local/hive/lib/zookeeper-3.4.5.jar,file:/usr/local/hive/=\nlib/hive-hbase-handler-0.13.1.jar,file:///usr/local/hive/lib/guava-11.0.2.j=\nar,file:///usr/local/hbase/lib/hbase-client-0.98.2-\nhadoop2.jar,file:///usr/local/hbase/lib/hbase-common-0.98.2-hadoop2.jar,fil=\ne:///usr/local/hbase/lib/hbase-protocol-0.98.2-hadoop2.jar,file:///usr/loca=\nl/hbase/lib/hbase-server-0.98.2-hadoop2.jar,file:///usr\n/local/hbase/lib/hbase-shell-0.98.2-hadoop2.jar,file:///usr/local/hbase/lib=\n/hbase-thrift-0.98.2-hadoop2.jar",
        "file:///var/hbase",
        "file:///var/hbase-data",
        "file:/export/hbase/cache.data",
        "file:/hadoop/tmp/dfs/data,file:/opt/tmp/dfs/data",
        "file:/hbase",
        "file:/home/cluster/mydata/hdfs/datanode",
        "file:/home/cluster/mydata/hdfs/namenode",
        "file:/home/hadoop/datanode",
        "file:/home/hadoop/namenode",
        "file:/home/hadoop3/tmp",
        "file:/home/hadoop3/tmp/dfs/data",
        "file:/home/hadoop3/tmp/dfs/name",
        "file:/home/hduser/yarn_data/hdfs/datanode",
        "file:/home/hduser/yarn_data/hdfs/namenode",
        "file:/home/software/data",
        "file:/home/software/name",
        "file:/home/xisisu/mydata/hdfs/datanode",
        "file:/home/xisisu/mydata/hdfs/namenode",
        "file:/home/yarn/mapred/local",
        "file:/home/yarn/mapred/system",
        "file:/mnt/foo/bar",
        "file:/ssddata6/hadoop/hadoop_store/hdfs/namenode",
        "file:/usr/local/hbase/hbase-0.94.2/data/hbase",
        "file:/var/data/hadoop/hdfs/dn",
        "file:/var/data/hadoop/hdfs/nn",
        "file:/work/hadoop/hadoop_store/hdfs/namenode",
        "filename1",
        "filename2",
        "foo",
        "foo,bar",
        "foo.bar.com:60000",
        "glusterfs://hbase-master:9000/hbase",
        "gs2,gs3,gs4",
        "h02,h03,h04,h05,h06,h07,h08,h09,h10",
        "h46,h47,h48",
        "hadmin",
        "hadoop",
        "hadoop,myuserapp supergroup,myuserapp",
        "hadoop,supergroup",
        "hadoop-coc-1:10020",
        "hadoop-coc-1:19888",
        "hadoop-hb",
        "hadoop-hb:60000",
        "hadoop-jh.mo-data.com",
        "hadoop-jh.mo-data.com:10200",
        "hadoop-master",
        "hadoop-master:8025",
        "hadoop-master:8030",
        "hadoop-master:8050",
        "hadoop-ps.mo-data.com:8080",
        "hadoop-rm.mo-data.com",
        "hadoop-rm.mo-data.com:8032",
        "hadoop.com",
        "hadoop0.rdpratti.com:8032",
        "hadoop1,hadoop2,hadoop3",
        "hadoop1,node1,node2,node2,node4",
        "hadoop1.local,node1.local,node2.local,node3.local,node4.local",
        "hadoop1.rad.wc.truecarcorp.com:10020",
        "hadoop1.rad.wc.truecarcorp.com:19888",
        "hadoop1.rad.wc.truecarcorp.com:8021",
        "hadoop1.rad.wc.truecarcorp.com:8030",
        "hadoop1.rad.wc.truecarcorp.com:8031",
        "hadoop1.rad.wc.truecarcorp.com:8032",
        "hadoop1.rad.wc.truecarcorp.com:8033",
        "hadoop1.rad.wc.truecarcorp.com:8088",
        "hadoop1:2181,hadoop2:2181,hadoop3:2181",
        "hadoop1:60000",
        "hadoop2:50070",
        "hadoop2:8020",
        "hadoop3:50070",
        "hadoop3:8020",
        "hadoop5:60000",
        "hadoopa.arindam.com:54311",
        "hadoopmaster:60000",
        "hb-trgt00",
        "hb_m_hdpnode5,60000,14304321126=\n54",
        "hb_m_hdpnode5,60000,=\n1430432112654",
        "hbase",
        "hbase-master",
        "hbase-master,hbase-regionserver1,hbase-regionserver2,hbase-regionserver3",
        "hbase-master:60000",
        "hbase/_HOST@EXAMPLE.COM",
        "hbase/_HOST@HADOOP.LAN",
        "hbasetest",
        "hbasezookeeper1.local,hbasezookeeper2.local,hbasezookeeper3.local,hbasezookeeper4.local,hbasezookeeper5.local",
        "hdfs",
        "hdfs\n://localhost:54310",
        "hdfs,hdpdadmngrp",
        "hdfs-cluster",
        "hdfs-master.local:60000",
        "hdfs/_HOST@BGDT.DEV.HRB",
        "hdfs/_HOST@RND.HDFS.COM",
        "hdfs/data",
        "hdfs/hpctest3.realm.com@REALM.COM",
        "hdfs/name",
        "hdfs/tmp",
        "hdfs://\n>> 172.16.100.13:9000",
        "hdfs:// 127.0.0.1:9000/hbase",
        "hdfs://${HDFS_SERVER}:${HDFS_PORT}/",
        "hdfs://${celer.hadoop.master.address}:${celer.hadoop.bind.port}",
        "hdfs://10.101.89.68:9000",
        "hdfs://10.103.0.17:9000",
        "hdfs://10.108.99.68:8020=\n",
        "hdfs://10.108.99.68:9001/home",
        "hdfs://10.108.99.69:9001/home",
        "hdfs://10.11.1.1:9000/hbase",
        "hdfs://10.14.24.19:9000/hbase",
        "hdfs://10.211.55.7:9000/hbase",
        "hdfs://10.255.1.61:8020/hbase",
        "hdfs://127.0.0.1:54310",
        "hdfs://127.0.0.1:9000",
        "hdfs://127.0.0.1:9000/hbase",
        "hdfs://127.0.0.1:9001",
        "hdfs://132.148.0.10:9001/ns1home",
        "hdfs://132.148.0.11:9001/ns2home",
        "hdfs://132.168.0.10:8020",
        "hdfs://132.168.0.10:8020/NN1Home",
        "hdfs://172.16.100.13:9000",
        "hdfs://172.16.100.5:9000",
        "hdfs://172.20.189.44:8020/hbase",
        "hdfs://172.20.193.234:9000",
        "hdfs://172.20.8.20:8020/hbase",
        "hdfs://172.28.1.138/hbase",
        "hdfs://191.168.1.102:9000/hbase",
        "hdfs://192.168.1.5:50000/hbase",
        "hdfs://192.168.1.68:9000/h=\nbase",
        "hdfs://192.168.1.68:9000/hbase",
        "hdfs://192.168.107.142:54310/hbase",
        "hdfs://192.168.11.150:8020/hbase",
        "hdfs://192.168.2.3:20001/hbase",
        "hdfs://192.168.20.30:9000/hbase",
        "hdfs://192.168.219.129:9000",
        "hdfs://192.168.253.130:60000",
        "hdfs://192.168.253.132:60000/hbase",
        "hdfs://192.168.33.204:11004/",
        "hdfs://192.168.33.204:11004/hbase",
        "hdfs://206.88.43.4:54311",
        "hdfs://206.88.43.8:54310",
        "hdfs://206.88.43.8:54310/hbase",
        "hdfs://58.155.50.240:8020/hbase",
        "hdfs://=\n192.168.1.68:9000/hbase",
        "hdfs://A:9000",
        "hdfs://ADDRESS:PORT",
        "hdfs://CH22:9000/hbase",
        "hdfs://CH22:9000/hbasenew",
        "hdfs://DC-TEST-1:9000/hbase",
        "hdfs://DNVR-DEV3.test.jsq.bsg.ad.adp.com:54310",
        "hdfs://Hadoop01:8020",
        "hdfs://Hadoopserver:54310/hbase",
        "hdfs://MACHINENAME:9000/hbase",
        "hdfs://RhCluster",
        "hdfs://atlas:54310/hbase",
        "hdfs://big7:54310/hbase",
        "hdfs://blabla-hadoop",
        "hdfs://blade1:9000/hbase",
        "hdfs://blr2211457.idc.oracle.com:8020/hbase",
        "hdfs://box1:9000",
        "hdfs://box1:9000/hbase",
        "hdfs://cdh1:54310/hbase",
        "hdfs://cent63V1.corp.ybusa.net:9000/hbase-${user.name}/hbase\n",
        "hdfs://cldx-1139-1033:9000",
        "hdfs://cldx-1139-1033:9000/hbase",
        "hdfs://clone11:9000/hbase",
        "hdfs://cloud15:54310",
        "hdfs://convera:9000/hbase",
        "hdfs://crunch:54310/hbase",
        "hdfs://dev32:9000/hbase",
        "hdfs://devrackA-00:8020",
        "hdfs://devrackA-00:8020/var/hbase-hadoop/hbase",
        "hdfs://devubuntu05:9000",
        "hdfs://domU-12-31-39-00-13-A1.compute-1.internal:50001",
        "hdfs://domU-12-31-39-00-25-C1.compute-1.internal:50001",
        "hdfs://domU-12-31-39-00-25-C1.compute-1.internal:50001/ \n> hbase",
        "hdfs://domU-12-31-39-00-25-C1.compute-1.internal:50001/ \nhbase",
        "hdfs://domU-12-31-39-00-25-C1.compute-1.internal:50001/hbase",
        "hdfs://domU-12-31-39-00-E0-96.compute-1.internal:54310/hbase",
        "hdfs://domU-12-31-39-00-E5-D2.compute-1.internal:50001",
        "hdfs://domU-12-31-39-00-E5-D2.compute-1.internal:50001/hbase",
        "hdfs://domU-12-31-39-00-E9-23:50001/hbase",
        "hdfs://domU-12-31-39=\n-00-E5-D2.compute-1.internal:50001",
        "hdfs://domU-15-35-19-0B-DD-29.compute-1.internal:9000/hbase",
        "hdfs://dynobuntu17:8020/hbase",
        "hdfs://ec2-184-73-22-146.compute-1.amazonaws.com/",
        "hdfs://ec2-23-23-33-234.compute-1.amazonaws.com:9010",
        "hdfs://ec2-23-23-33-234.compute-1.amazonaws.com:9010\n> ",
        "hdfs://ec2-54-226-206-28.compute-1.amazonaws.com:9010/hbase",
        "hdfs://ec2-54-234-17-36.compute-1.amazonaws.com:9010/hbase",
        "hdfs://ec2-54-234-17-36.compute-1.amazonaws.com:901=\n0/hbase",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbas\n> >>>>>> e\n> >>>>>> ",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n> ",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n> > ",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n> > > ",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n>> > ",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n>>> ",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n>>>> ",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n>>>>> ",
        "hdfs://ec2-67-202-24-167.compute-1.amazonaws.com:9000/hbase\n>>>>>> ",
        "hdfs://elroy/hbase",
        "hdfs://fs0.cm.cluster:8020",
        "hdfs://fs0.cm.cluster:8020/hbase",
        "hdfs://h-24-30.sfo.stumble.net:8020/hbase",
        "hdfs://h46:9000",
        "hdfs://h46:9000/hbase",
        "hdfs://hadoop-namenode:9000/hbase",
        "hdfs://hadoop-nn.mo-data.com:9000/",
        "hdfs://hadoop0.rdpratti.com:8020",
        "hdfs://hadoop1/mapred",
        "hdfs://hadoop1:9000/hbase",
        "hdfs://hadoop:54310/hbase",
        "hdfs://hadoopa.arindam.com:54310",
        "hdfs://hadoopmaster:9000/hbase",
        "hdfs://hbaseserver:50001/hbase",
        "hdfs://hbasetest:9000/hbase",
        "hdfs://hdfs-master.local:50070/hbase",
        "hdfs://hornet-master01",
        "hdfs://hornet-master01:8020/hbase",
        "hdfs://infinity1:9000",
        "hdfs://infinity1:9000/hbase",
        "hdfs://intelliserver:54310",
        "hdfs://kdc.hadoop.lan:8020/hbase",
        "hdfs://kyahadmaster:54310",
        "hdfs://linux76:9000/hbase",
        "hdfs://lklcluster",
        "hdfs://loca=\nlhost:10011/hbase",
        "hdfs://localhost",
        "hdfs://localhost/",
        "hdfs://localhost/hbase",
        "hdfs://localhost:10011/hbase",
        "hdfs://localhost:10=\n011/hbase",
        "hdfs://localhost:24400/hbase",
        "hdfs://localhost:24400/hbase-staging",
        "hdfs://localhost:24400/zookeeper",
        "hdfs://localhost:50009/hbase",
        "hdfs://localhost:54310",
        "hdfs://localhost:54310/hbase",
        "hdfs://localhost:54310/home/hadoop/Desktop/hbasedata",
        "hdfs://localhost:54311",
        "hdfs://localhost:8020",
        "hdfs://localhost:8020/hbase",
        "hdfs://localhost:9000",
        "hdfs://localhost:9000/",
        "hdfs://localhost:9000/Hadoop/hbase-0.1.1",
        "hdfs://localhost:9000/accumulo",
        "hdfs://localhost:9000/hbase",
        "hdfs://localhost:9000/hdfs",
        "hdfs://localhost:9001/hbase",
        "hdfs://master.bigdata.com:54310/hbase",
        "hdfs://master.cluster.org:54310/",
        "hdfs://master.cluster.org:54310/hbase",
        "hdfs://master.hadoopcluster:9000/hbase",
        "hdfs://master/hbase",
        "hdfs://master2:9000",
        "hdfs://master:54310",
        "hdfs://master:54310/hbase",
        "hdfs://master:8020",
        "hdfs://master:8020/hbase",
        "hdfs://master:9000",
        "hdfs://master:9000/hbase",
        "hdfs://master:9000/user/hadoop/hbase",
        "hdfs://masterhost:9000",
        "hdfs://masterhost:9000/hbase",
        "hdfs://my.hbase.server.de:8020/hbase",
        "hdfs://mycluser:8020",
        "hdfs://mycluster",
        "hdfs://mycluster/hbase",
        "hdfs://mycluster:8032",
        "hdfs://namenode/hbase",
        "hdfs://namenode01.8020/datausers",
        "hdfs://namenode02:8020/apachelogs",
        "hdfs://namenode:54310",
        "hdfs://namenode:54310/",
        "hdfs://namenode:8020/hbase",
        "hdfs://namenode:9000/hbase",
        "hdfs://nameservice1",
        "hdfs://newcloud/hbase",
        "hdfs://node1/",
        "hdfs://node1/hbase",
        "hdfs://node1:8020/apps/hbase/data",
        "hdfs://node1:8020/hbase",
        "hdfs://node3:54310/hbase",
        "hdfs://node3:9000/",
        "hdfs://node3:9000/hbase",
        "hdfs://ns1",
        "hdfs://ns1/dir1",
        "hdfs://ns2/dir2",
        "hdfs://ns3/dir3",
        "hdfs://ns4/dir4",
        "hdfs://obeli=\nx8.local:9001/hbase",
        "hdfs://obelix8.local:9001/hbase",
        "hdfs://orahadoop",
        "hdfs://psyDebian:9000",
        "hdfs://psyDebian:9000/hbase",
        "hdfs://public-cluster",
        "hdfs://rajeevks-dx.bagmane.corp.yahoo.com:9000/hbase",
        "hdfs://s1.idfs.cn:9000",
        "hdfs://s1.idfs.cn:9000/hbase",
        "hdfs://s3:9000",
        "hdfs://samplehost.com:9000",
        "hdfs://sb-centercluster01:9000",
        "hdfs://sb-centercluster01:9000/hbase",
        "hdfs://sb-centercluster01:9100",
        "hdfs://sb-centercluster01:9100/hbase",
        "hdfs://sb-centercluster01:9101",
        "hdfs://search9b.cm3:9000",
        "hdfs://search9b.cm3:9000/hbase",
        "hdfs://server1.mydomain.com:9000",
        "hdfs://server1:50070/hbase",
        "hdfs://server1:9000/hbase",
        "hdfs://server3.yun.com:54310/hbase",
        "hdfs://server:8020",
        "hdfs://server_1:54310/hbase",
        "hdfs://servername:8020",
        "hdfs://sha-cs-04:9000/hbase",
        "hdfs://sk.r252.0:54310",
        "hdfs://slave2:9000",
        "hdfs://sns12.cs.princeton.edu:8020/hbase",
        "hdfs://syed:9000/hbase",
        "hdfs://sz:8998/hbase",
        "hdfs://test-cluster",
        "hdfs://tobethink.pappiptek.lipi.go.id:38400/user/hadoop/hbase",
        "hdfs://tobethink.pappiptek.lipi.go.id:54310/user/hadoop/hbase",
        "hdfs://tobethink.pappiptek.lipi.go.id:54310/user/hadoop/hbase\n> ",
        "hdfs://tobethink.pappiptek.lipi.go.id:54310/user/hadoop/hbase\n>> ",
        "hdfs://ub13:54310/hbase",
        "hdfs://ubuntu6:9000/hbase",
        "hdfs://user-laptop:9000/hbase",
        "hdfs://xxxxx.yyyy.com:9920",
        "hdfs://xxxxx/hbase",
        "hdfs://ycsb1:20001/hbase",
        "hdfs://{your machine name} or {localhost}:9000/hbase",
        "hdfs:/=\n/intelliserver:54310",
        "hdpdadmngrp",
        "hdpnode1,hdpnode2,hdpno=\nde3,hdpnode4,hdpnode5",
        "hive",
        "hiveuser1",
        "home/wanli/hadoop/hbasezook",
        "hornet-master01,hornet-master02,hornet-master03",
        "host/_HOST@......",
        "host/_HOST@BGDT.DEV.HRB",
        "host/_HOST@RND.HDFS.COM",
        "hostname.dc.xx.org:50010",
        "hostname.dc.xx.org:50020",
        "hostname.dc.xx.org:50075",
        "hpctest3.realm.com:50070",
        "hpctest3.realm.com:50090",
        "http/_HOST@BGDT.DEV.HRB",
        "http://192.168.56.101:19888/jobhistory/logs/",
        "http://localhost:60000",
        "http://server:",
        "http://tobethink.pappiptek.lipi.go.id::60000",
        "http://xxxx:19888/jobhistory/logs/",
        "ihub-jobtracker1",
        "infinity1",
        "infinity1:60000",
        "infinity1:9001",
        "intelliserver:54311",
        "ip-10-10-100170.eu-east-1.compute.internal:8021",
        "ip-10-159-41-177.ec2.intern=\nal",
        "ip-10-159-41-177.ec2.internal",
        "ip-10-178-13-39.ec2.internal",
        "ip-AA-BBB-C-DDD.ec2.internal",
        "ip-XX-YYY-Z-QQQ.ec2.internal",
        "ip-XX-YYY-Z-QQQ.ec2.internal.ec2.internal",
        "ip-xx-xx-xx-xx.compute.internal",
        "java.=\n,javax.,org.apache.commons.logging.,org.apache.log4j.,\n          org.apache.hadoop.,core-default.xml,hdfs-default.xml,\n          mapred-default.xml,yarn-default.xml",
        "jbmnode02.jibemobile.jp:50470",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=3DHadoop@234\n>\n> ",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=3DHadoop@234\n> >\n> > ",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=3DHadoop@234\n>> ",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=3DHadoop@234\n>>>=20\n>>> ",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=3DHadoop@23=\n4\n>>>>>=20\n>>>>> ",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=3DHadoop@2=\n34\n> > >\n> > > ",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=Hadoop@234\n> ",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=Hadoop@234\n>> > >\n>> > > ",
        "jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=Hadoop@234\n>>>>\n>>>> ",
        "jdbc:mysql://localhost/hivemeta?createDatabaseIfNotExist=true",
        "jdbc:mysql://localhost:3306/hadoop?createDatabaseIfNotExist=3Dtr=\nue",
        "jdbc:mysql://localhost:3306/hadoop?createDatabaseIfNotExist=3Dtru=\ne",
        "jdbc:mysql://localhost:3306/hadoop?createDatabaseIfNotExist=3Dtrue=\n",
        "jdbc:mysql://localhost:3306/metastore?createDatabaseIfNotExist=true",
        "jhs/_HOST@REALM=\n.TLD",
        "jobtracker.ld.net:50030",
        "jobtracker.ld.net:8021",
        "kdc.hadoop.lan",
        "kerberos",
        "ks25937.kimsufi.com",
        "lklcluster",
        "lo0",
        "local",
        "localhost",
        "localhost.localdomain",
        "localhost:10020",
        "localhost:13562",
        "localhost:19888",
        "localhost:47111",
        "localhost:54311",
        "localhost:60000",
        "localhost:60010",
        "localhost:60020",
        "localhost:8021",
        "localhost:9000",
        "localhost:9001",
        "localhost:9002",
        "log=\ns",
        "logs",
        "lzo",
        "m146,m145,m143",
        "m146:50090",
        "machine1.mydomain.com:54311",
        "mapred.job.queue.name",
        "mapred/_HOST@RND.HDFS.COM",
        "mapreduce.shuffle",
        "mapreduce_shuffle",
        "master",
        "master,slave,serverslave",
        "master,slave1,slave2",
        "master.bigdata.com,slave1.bigdata.com,slave2.bigda=\nta.com,\n>> slave3.bigdata.com",
        "master.bigdata.com,slave1.bigdata.com,slave2.bigdata.com,\n> > slave3.bigdata.com",
        "master.bigdata.com,slave1.bigdata.com,slave2.bigdata.com,\n> slave3.bigdata.com",
        "master.bigdata.com,slave1.bigdata.com,slave2.bigdata.com,\n>>> slave3.bigdata.com",
        "master.bigdata.com,slave1.bigdata.com,slave2.bigdata.com,\nslave3.bigdata.com",
        "master.cluster.org",
        "master.hadoop.local,host49.hadoop.local",
        "master.hadoopcluster,slave1.hadoopcluster,slave2.hadoopcluster",
        "master2:18025",
        "master2:18030",
        "master2:18040",
        "master2:18141",
        "master:50090",
        "master:54311",
        "master:60000",
        "master:60400",
        "master:8030",
        "master:8032",
        "master:8033",
        "master:8035",
        "master:8050",
        "master:8088",
        "master:8990",
        "master:8991",
        "master:8993",
        "master:8994",
        "master:9000",
        "master:9001",
        "master:9002",
        "master:9003",
        "masterhost:60000",
        "mb0:9000",
        "mb0:9001",
        "multiwal",
        "murmur",
        "my-zookeeper-machine.net",
        "my.hbase.server.de",
        "myEndpointImpl",
        "myHostName",
        "myauxserviceclassname",
        "mycluster",
        "myhdfsgroup",
        "mykeyid",
        "mysecret",
        "namenode",
        "namenode, jobtracker, slave0",
        "namenode.ld.net:50070",
        "namenode1,namenode2",
        "namenode1:50070",
        "namenode1:8020",
        "namenode1:9000",
        "namenode2:50070",
        "namenode2:8020",
        "namenode2:9000",
        "namenode38,namenode90",
        "namenode3:9000",
        "namenode4:9000",
        "namenode:50070",
        "nameservice1",
        "nn.domain:8020",
        "nn1,nn2",
        "nn1,nn2,nn3",
        "nn1,nn3",
        "nn1:50070",
        "nn1:8020",
        "nn2,nn4",
        "nn2:50070",
        "nn2:8020",
        "node-01,node-02,node-03,node-04,node-05",
        "node01.example.com:50070",
        "node01.example.com:50470",
        "node01.example.com:8020",
        "node02.example.com:50070",
        "node02.example.com:8020",
        "node1",
        "node1,node2",
        "node1,node2,node3",
        "node1.local,node2.local,node3.local,node4.local",
        "node1:18025",
        "node1:18030",
        "node1:18040",
        "node1:18088",
        "node1:50070",
        "node1:8020",
        "node2:50070",
        "node2:8020",
        "node2:9000",
        "node3,hadoop5,hadoopoffice85,hadoopoffice88,hdofficelj001",
        "none",
        "ns1,ns2",
        "ns1=2Cns2",
        "obelix105.local,obelix106.local,obelix107.local",
        "obelix8.local:60000",
        "offheap",
        "orahadoop",
        "org.a=\npache.hadoop.mapred.MapTask$MapOutputBuffer",
        "org.apache.h=\nadoop.hbase.master.cleaner.HFileLinkCleaner,org.apache.hadoop.hbase.master.=\nsnapshot.SnapshotHFileCleaner,org.apache.hadoop.hbase.master.cleaner.TimeTo=\nLiveHFileCleaner",
        "org.apache.h=\nadoop.yarn.server.timeline.LeveldbTimelineStore",
        "org.apache.had=\noop.fs.Hdfs",
        "org.apache.had=\noop.hbase.master.balancer.StochasticLoadBalancer",
        "org.apache.had=\noop.hbase.master.cleaner.TimeToLiveLogCleaner,org.apache.hadoop.hbase.repli=\ncation.master.ReplicationLogCleaner,org.apache.hadoop.hbase.master.snapshot=\n.SnapshotLogCleaner",
        "org.apache.hado=\nop.fs.HarFs",
        "org.apache.hadoop.=\nhbase.client.ClusterStatusListener$MulticastListener",
        "org.apache.hadoop.fs.s3a.S3AFileSy=\nstem",
        "org.apache.hadoop.fs.s3native.NativeS3FileSystem",
        "org.apache.hadoop.hba=\nse.regionserver.tableindexed.IndexedRegionServer",
        "org.apache.hadoop.hbase.coprocessor.AggregateImplementation",
        "org.apache.hadoop.hbase.coprocessor.SampleRegionWALObserver",
        "org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider",
        "org.apache.hadoop.hbase.ipc.HRegionInterface",
        "org.apache.hadoop.hbase.ipc.IndexedRegionInterf=\nace",
        "org.apache.hadoop.hbase.ipc.IndexedRegionInterfac=\ne",
        "org.apache.hadoop.hbase.ipc.IndexedRegionInterface",
        "org.apache.hadoop.hbase.ipc.IndexedRegionInterface=\n",
        "org.apache.hadoop.hbase.ipc.SecureRpcEngine",
        "org.apache.hadoop.hbase.ipc.TransactionalRegionInterface",
        "org.apache.hadoop.hbase.mapred.TableInputFormat",
        "org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner",
        "org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy",
        "org.apache.hadoop.hbase.regionserver.IdxRegion",
        "org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy",
        "org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRe=\ngionServer",
        "org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionS=\nerver",
        "org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionSe=\nrver",
        "org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionSer=\nver",
        "org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServe=\nr",
        "org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer",
        "org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogRead=\ne\n> > >\n> > > r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReade\n>\n> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReade\n> >\n> > r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReade\n>> > >\n>> > > r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReade\n>> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReade\n>>>=20\n>>> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReade\n>>>>\n>>>> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReade=\n\n>>>>>=20\n>>>>> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader\n> ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader\n> > ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader\n> > >> > ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader\n> >> > ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader\n>> > ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader=\n",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWrit=\ne\n> > >\n> > > r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWrite\n>\n> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWrite\n> >\n> > r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWrite\n>> > >\n>> > > r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWrite\n>> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWrite\n>>>=20\n>>> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWrite\n>>>>\n>>>> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWrite=\n\n>>>>>=20\n>>>>> r",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter\n> ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter\n> > ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter\n> > >> > ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter\n> >> > ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter\n>> > ",
        "org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter=\n",
        "org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.access.AccessController,org.apache.hadoop.hbase.security.visibility.VisibilityController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,\n           org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,\n>            org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,\n> >\n>  org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,\n> > >\n> >  org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,\n> > > >\n> > >  org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,\norg.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,org.apach=\ne.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hba=\nse.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.h=\nadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.h=\nadoop.hbase.security.access.AccessController,org.apache.hadoop.hbase.securi=\nty.access.SecureBulkLoadEndpoint",
        "org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.had=\noop.hbase.security.access.AccessController,org.apache.hadoop.hbase.security=\n.access.SecureBulkLoadEndpoint",
        "org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hbase.security.access.AccessController,org.apache.hadoop.hbase.security.visibility.VisibilityController",
        "org.apache.hadoop.hbase.security.visibility.VisibilityController",
        "org.apache.hadoop.hbase.security.visibility.VisibilityController,org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hbase.security.visibility.VisibilityController,org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hbase.security.access.AccessController",
        "org.apache.hadoop.hdfs.server.namenode.AzureBlockPlacementPolicy=\n",
        "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxy=\nProvider",
        "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyP=\nrovider",
        "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyPr\n>ovider",
        "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyPr=\novider",
        "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
        "org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.se\n>curity.authentication.server.AuthenticationFilter\n>",
        "org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.security.AuthenticationFilterInitializer",
        "org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.security.authentication.server.AuthenticationFilter \n",
        "org.apache.hadoop.io.compress.DefaultCodec",
        "org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.co=\nmpress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compres=\nsion.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec",
        "org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec",
        "org.apache.hadoop.io.compress.GzipCodec",
        "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.com=\npress.DefaultCodec",
        "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compr=\ness.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec",
        "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compre=\nss.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec",
        "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec",
        "org.apache.hadoop.io.compress.SnappyCodec",
        "org.apache.hadoop.io.seriali=\nzer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecific=\nSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization=\n",
        "org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization",
        "org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization,org.apache.hadoop.io.serializer.JavaSerialization",
        "org.apache.hadoop.mapred.FairScheduler",
        "org.apache.hadoop.mapred.JobQueueTaskScheduler",
        "org.apache.hadoop.mapred.ShuffleHandler",
        "org.apache.hadoop.net.Networ=\nkTopology",
        "org.apache.hadoop.net.ScriptBasedMapping",
        "org.apache.hadoop.net.SocksSocketFactory",
        "org.apache.hadoop.security.AuthenticationFilterInitializer",
        "org.apache.hadoop.thriftfs.ThriftJobTrackerPlugin",
        "org.apache.hadoop.util.QuickSor=\nt",
        "org.apache.hadoop.yarn.ipc.=\nHadoopYarnProtoRPC",
        "org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor",
        "org.apache.hadoop.yarn.server.nodemanager.util.DefaultL=\nCEResourcesHandler",
        "org.apache.hadoop.yarn.server.resourcemanager.resource.DefaultResourceCalculator",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.=\nCapacityScheduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.C=\napacityScheduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.Cap=\nacityScheduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Fai=\nrScheduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Fair=\nScheduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairS=\ncheduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSch=\neduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSche\nduler",
        "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler",
        "org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin",
        "org.apache.hadoop.yarn.util.resource.\n> DominantResourceCalculator ",
        "org.apache.hadoop=\n.hbase.master.ClusterStatusPublisher$MulticastPublisher",
        "org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer",
        "org.apache.phoenix.hbase.index.master.IndexMasterObserver",
        "org.apache=\n.hadoop.hbase.regionserver.wal.ProtobufLogReader",
        "org.apache=\n.hadoop.hbase.regionserver.wal.ProtobufLogWriter",
        "org.apache=\n.hadoop.io.compress.DefaultCodec",
        "org.apache=\n.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider",
        "org.cloudera.htrace.impl.LocalFileSpanReceiver",
        "org.cloudera.htrace.impl.ZipkinSpanReceiver",
        "org.htrace.impl.HBaseSpanReceiver",
        "org.htrace.impl.LocalFileSpanReceiver",
        "org.spaggiari.hbase.RegionServerPerformanceBalancer",
        "org=\n.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager",
        "paramValue",
        "path_to_your_datadir",
        "pc225.emulab.net,pc273.emulab.net,pc210.emulab.net",
        "pc228:54311",
        "pnjhadoopnn01:50070",
        "pnjhadoopnn01:8020",
        "privacy",
        "production,exploration",
        "psyDebian,centos1,centos2",
        "psyDebian:60000",
        "public-cluster",
        "qjournal://10.0.0.144:8485;10.0.0.145:8485;10.0.0.146:8485/public-=\ncluster",
        "qjournal://10.0.10.5:8485;10.0.10.6:8485;10.0.10.7:8485\n> ;10.0.10.8:8485;10.0.10.9:8485;10.0.10.10:8485;10.0.10.11:8485/yisabigdata\n> ",
        "qjournal://10.232.98.61:20022;10.232.98.62:20022;\n> 10.232.98.63:20022/nn1,nn2",
        "qjournal://10.232.98.61:20022;10.232.98.62:20022;\n>> 10.232.98.63:20022/nn1,nn2",
        "qjournal://10.232.98.61:20022;10.232.98.62:20022;10.232.98.63:20022/nn1,nn2",
        "qjournal://192.168.219.129:8485;192.168.219.132:8485;192.168.219.1=\n30:8485/RhCluster",
        "qjournal://GS-CIX-SEV0001:8485;GS-CIX-SEV0002:8485;GS-CIX-SEV0003:84=\n85/commoncluster",
        "qjournal://GS-CIX-SEV0001:8485;GS-CIX-SEV0002:8485;GS-CIX-SEV0003:84=\n85/hbasecluster",
        "qjournal://bgdt01.dev.hrb:8485;bgdt03.dev.hrb:8485;bgdt04.dev.hrb:=\n8485/bgdt-dev-hrb",
        "qjournal://bgdt01.dev.hrb:8485;bgdt03.dev.hrb:8485;bgdt04.dev.hrb=\n:8485/bgdt-dev-hrb",
        "qjournal://clone3:8485;clone1:8485;clone2:8485/orahadoop",
        "qjournal://datanode2:8485;datanode3:8485;datanode4:8485/ns1",
        "qjournal://hadoop1:8485;hadoop2:8485;hadoop3:8485/lklcluster",
        "qjournal://journalnode1:8485;journalnode2:8485;journalnode3:8485/hd=\nfs-cluster",
        "qjournal://n1.com:8485;n2.com:8485/cluster1",
        "qjournal://n1.com:8485;n2.com:8485/cluster2",
        "qjournal://node1.example.com:8485;node2.example.com:8485;node3.example.com:8485/clustera",
        "qjournal://node1.example.com:8485;node2.example.com:8485;node3.example.com:8485/clusterb",
        "qjournal://node1:8485;node2:8485;node3:8485/mycluster",
        "quobyte://\n> prod.corp.quobyte.com:7861/users/kaisers/hadoop-test/",
        "quobyte://\nprod.corp.quobyte.com:7861/users/kaisers/hadoop-test/",
        "rack3:8021",
        "rajeevks-dx.bagmane.corp.yahoo.com:60000",
        "rc4",
        "root-region-server",
        "roundrobindnsdnsentry",
        "rs1.example.com,rs2.example.com,rs3.example.com,\n> rs4.example.com,rs5.example.com",
        "rs1.example.com,rs2.example.com,rs3.example.com,rs4.example.co=\nm,rs5.example.com",
        "s3://bucket1",
        "s3://bucketname/hbase-tmp/",
        "s3://bucketname/hbasebackup/",
        "s3://hbase2.s3.amazonaws.com:80/hbasedata",
        "s3://hbase20:80/hbasedata",
        "s3n://bucketname",
        "s3n://mcorner-hbase/",
        "s3n://mybkt",
        "s4:9001",
        "samplehost.com:9001",
        "sandbox.com:19888",
        "sandbox.com:8141",
        "sb-centercluster01:60002",
        "sb-centercluster01:9000",
        "sb-centercluster01:9001",
        "search58c.cm3,build13.cm3,build14.cm3",
        "search9b.cm3:9001",
        "secnamenode.ld.net:50090",
        "secondarynamenode:50090",
        "server1",
        "server1,serve3,server5",
        "server1:60000",
        "server3.yun.com",
        "server_2",
        "sha-cs-01,sha-cs-02,sha-cs-03,sha-cs-05,sha-cs-06",
        "sha-cs-01,sha-cs-02,sha-cs-03,sha-cs-05,sha-cs-0=\n6",
        "sha-cs-01,sha-cs-02,sha-cs-03,sha-cs-05,sha-cs=\n-06",
        "sha-cs-01,sha-cs-02,sha-cs-03,sha-cs-0=\n5,sha-cs-06",
        "shell(/bin/true)",
        "shouldnevereverevermatch",
        "simple",
        "sk.r252.0:54311",
        "slave",
        "slave2:9001",
        "slc00dgd:10020",
        "slc00dgd:19888",
        "sles-hdfs1:50070",
        "sles-hdfs1:9000",
        "sles-hdfs2,sles-hdfs5",
        "sles-hdfs2:50070",
        "sles-hdfs4:9000",
        "snappy",
        "snappy,gz",
        "snn-host1:http-port",
        "snn.domain:8020",
        "sns12-virt.CS.Princeton.EDU,sns13-virt.CS.Princeton.EDU,sns14-virt.CS.Princeton.EDU",
        "sns12-virt.CS.Princeton.EDU,sns13-virt.CS.Princeton.EDU,sns14-virt=\n.CS.Princeton.EDU",
        "sns12-virt.CS.Princeton.EDU:60000",
        "sns12-virt.CS.Princeton.EDU=2Csns13-virt.CS.Princeton.EDU=2Csns14-=\nvirt.CS.Princeton.EDU",
        "some-server",
        "sqoop,defau=\nlt",
        "sqoop,default",
        "srikanth-laptop:60000",
        "sshfence",
        "sshfence\n>                shell(/bin/true)\n>         ",
        "sshfence\n>                shell(q_hadoop_fence.sh $target_host $target_port)\n>         ",
        "sshfence\n> >                shell(/bin/true)\n> >         ",
        "sshfence\n> >                shell(q_hadoop_fence.sh $target_host $target_port)\n> >         ",
        "sshfence\n> shell(/bin/true)\n> ",
        "sshfence\n>> shell(/bin/true)\n>> ",
        "ssl-client.=\nxml",
        "ssl-server.=\nxml",
        "supergroup",
        "syed",
        "syed:60000",
        "system-nati=\nve",
        "sz,hadoop3",
        "sz:60000",
        "t=\nrue",
        "target/classes/META-INF/mysql-connector-java-3.1.12-bin.jar",
        "test",
        "test1",
        "testuser",
        "thrift://localhost.localdomain:10000",
        "thrift://localhost.localdomain:1000=\n0",
        "tr=\nue",
        "tru=\ne",
        "true",
        "true=\n",
        "ub13,ub12,ub11",
        "ubuntu2,ubuntu3,ubuntu7,ubuntu9,ubuntu6",
        "user-laptop",
        "usera, userb",
        "usera, userc",
        "userb",
        "userc",
        "vamshi",
        "vamshi,vamshi_RS",
        "vamshi_RS",
        "viewfs:///",
        "vm38.dbweb.ee:8089",
        "webuser,supergroup",
        "webuser,webgroup",
        "world:anyone:rwcda=\n",
        "xxxxx",
        "xxxxx.yyyy.com:50070",
        "xxxxx.yyyy.com:50470",
        "yarn",
        "yarn,mapred",
        "ycsb1",
        "ycsb1:60000",
        "yisabigdata",
        "you_host_or_ip",
        "zk1",
        "zk1,zk2,zk3",
        "zookeeper-1,zookeeper-2,zookeeper-3,",
        "zookeeper:2181",
        "{your machine name} or {localhost}",
        "{your machine name} or {localhost}:60000",
        "~/hbase/tempandlog/temp",
        "~/hbase/tempandlog/zookeeper"
    ]
}